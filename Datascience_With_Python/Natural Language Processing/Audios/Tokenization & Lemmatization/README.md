## Data Science with Python : Tokenization & Lemmatization

## [Click here to listen the audio](https://drive.google.com/file/d/1gOCuApfqKWPQX7E5aOIlNNyXq6kDbVC1/view?usp=sharing)

**Tokenization**
- It is a common task in NLP.
- It is the building blocks of NLP.
- It is the way of separating a text into smaller units called “tokens”.

![Capture_1](https://user-images.githubusercontent.com/79050917/135420725-1759fecc-5888-462c-b8dd-2071a4760208.PNG)


**Types of Tokenization:**

- Tweet Tokenizer:
Specifically designed for tokenizing tweets.

- MWE tokenizer:
Multi-Word Expression.
Certain group of multiple words are treated as one entity during tokenization.

- Regular Expression tokenizer
Developed using regular expression.
Sentence are split based on occurrence of a pattern.

- Whitespace Tokenizer:
Splits a string whenever a space, tab, or newline character is present.

- Word Punkt Tokenizer:
Splits text into a list of characters and digits.

**Why tokenization? Or advantages.**

- We can easily apply the NLP models.
- Easy to evaluate the text.
- Easy to create word clouds.
- Easy to implement pre-processing techniques like Lemmatization, stemming, stop word removal.

**Lemmatization:**

- It derives the root word in a text.

![Capture_2](https://user-images.githubusercontent.com/79050917/135420690-d105fa78-8887-4f94-ab47-f737d56a805a.PNG)


**Why Lemmatization? Or Advantages.**

- Helps to reduce the length of words.
- Increases the model performs.
- Decreases the training time.
- Improves the accuracy. 
