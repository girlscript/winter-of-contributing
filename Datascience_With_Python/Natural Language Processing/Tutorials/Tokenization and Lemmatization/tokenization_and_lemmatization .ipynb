{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization and Lemmatization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZN63Xcx9fDc"
      },
      "source": [
        "#**Natural Language Processing**\n",
        "\n",
        "Natural Language Processing (NLP) is a subfield of computer science, artificial intelligence, information engineering, and human-computer interaction. This field focuses on how to program computers to process and analyze large amounts of natural language data. It is difficult to perform as the process of reading and understanding languages is far more complex than it seems at first glance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1] **Tokenization**\n",
        "\n",
        "- By tokenizing, you can conveniently split up text by word or by sentence.\n",
        "\n",
        "-  This will allow you to work with smaller pieces of text that are still relatively coherent and meaningful even outside of the context of the rest of the text. \n",
        "\n",
        "- It’s the  first step in turning unstructured data into structured data, which is easier to analyze.\n",
        "\n",
        "- When you’re analyzing text, you’ll be tokenizing by word and tokenizing by sentence. \n",
        "\n",
        "Natural Language toolkit has very important module NLTK tokenize sentences which further comprises of sub-modules\n",
        "\n",
        "- word tokenize\n",
        "- sentence tokenize\n",
        "\n",
        "\n",
        "#1) **Tokenizing by word:**\n",
        "\n",
        " - Words are like the atoms of natural language.\n",
        " - They’re the smallest unit of meaning that still makes sense on its own.\n",
        " - Tokenizing your text by word allows you to identify words that come up particularly often.\n",
        " \n",
        " \n",
        "\n",
        "># **word_tokenize module**\n",
        "\n",
        "word_tokenize module is used for basic word tokenization. \n",
        "\n",
        "Following example will use this module to split a sentence into words.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    import nltk\n",
        "\n",
        "    from nltk.tokenize import word_tokenize\n",
        "\n",
        "    text=\"GWOC is one of the best Open Source Community\"\n",
        "\n",
        "    print(word_tokenize(text))\n",
        "\n",
        "**Output:**\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    ['GWOC', 'is', 'one', 'of', 'the', 'best', 'Open ', 'Source', 'Community']\n",
        "\n",
        "\n",
        "**Explanation:** \n",
        "\n",
        "1. word_tokenize module is imported from the NLTK library.\n",
        "\n",
        "2.  A variable “text” is initialized.\n",
        "\n",
        "3. Text variable is passed in word_tokenize module and printed the result.\n",
        "\n",
        "4. This module breaks each word with punctuation which you can see in the output.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#2) **Tokenizing by sentence:**\n",
        "\n",
        "-  Sub-module available for the above is sent_tokenize. \n",
        "\n",
        "- An obvious question in your mind would be why sentence tokenization is needed when we have the option of word tokenization. Imagine you need to count average words per sentence, how you will calculate?\n",
        "\n",
        "-  For accomplishing such a task, you need both NLTK sentence tokenizer as well as NLTK word tokenizer to calculate the ratio. Such output serves as an important feature for machine training as the answer would be numeric.\n",
        "\n",
        "># **sentence_tokenize module:**\n",
        "\n",
        "\n",
        "**Example:**\n",
        "\n",
        "\n",
        "\n",
        "    import nltk\n",
        "\n",
        "    from nltk.tokenize import sent_tokenize\n",
        "\n",
        "    text = \"God is Great! I won a lottery.\"\n",
        "\n",
        "    print(sent_tokenize(text))\n",
        "\n",
        "**Output:** \n",
        "\n",
        "    ['God is Great!', 'I won a lottery ']\n",
        "\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "\n",
        "1. In a line like the previous program, imported the sent_tokenize module.\n",
        "2. Further sentence tokenizer in NLTK module parsed that sentences and show output. It is clear that this function breaks each sentence.\n",
        "\n",
        "Above word tokenizer example is good settings stones to understand the mechanics of the word and sentence tokenization.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#**2] Lemmatization** \n",
        "\n",
        "- Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item.\n",
        "\n",
        "-  Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meanings to one word. \n",
        "\n",
        "- Text preprocessing includes both Stemming as well as Lemmatization. Many times people find these two terms confusing. Some treat these two as the same.\n",
        "\n",
        "- Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.\n",
        "\n",
        "#**Applications of lemmatization are:** \n",
        " \n",
        "1. Used in comprehensive retrieval systems like search engines.\n",
        "\n",
        "2. Used in compact indexing\n",
        " \n",
        "\n",
        "#**Examples of lemmatization:**\n",
        "\n",
        "-> rocks : rock\n",
        "\n",
        "-> corpora : corpus\n",
        "\n",
        "-> better : good\n",
        "\n",
        "One major difference with stemming is that lemmatize takes a part of speech parameter, “pos” If not supplied, the default is “noun.”\n",
        "\n",
        "#**Below is the implementation of lemmatization words using NLTK:**\n",
        "\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        " \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        " \n",
        "    print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "\n",
        "    print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        " \n",
        "    print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n",
        "\n",
        "#**Output:**\n",
        " \n",
        "    rocks : rock\n",
        "\n",
        "    corpora : corpus\n",
        "\n",
        "    better : good\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci20fhwh9kgM"
      },
      "source": [
        ""
      ]
    }
  ]
}
