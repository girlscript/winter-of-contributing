{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Reinforcement Learning Chess using Temporal Difference Learning."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\r\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import inspect"
   ],
   "outputs": [],
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "!pip install --upgrade git+https://github.com/arjangroen/RLC.git  # RLC is the Reinforcement Learning package"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting git+https://github.com/arjangroen/RLC.git\r\n",
      "  Cloning https://github.com/arjangroen/RLC.git to /tmp/pip-req-build-gn6xg08b\r\n",
      "  Running command git clone -q https://github.com/arjangroen/RLC.git /tmp/pip-req-build-gn6xg08b\r\n",
      "Building wheels for collected packages: RLC\r\n",
      "  Building wheel for RLC (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-x011tmyw/wheels/04/68/a5/cb835cd3d76a49de696a942739c71a56bfe66d0d8ea7b4b446\r\n",
      "Successfully built RLC\r\n",
      "Installing collected packages: RLC\r\n",
      "Successfully installed RLC-0.3\r\n"
     ]
    }
   ],
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from RLC.move_chess.environment import Board\r\n",
    "from RLC.move_chess.agent import Piece\r\n",
    "from RLC.move_chess.learn import Reinforce"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "env = Board()\r\n",
    "env.render()\r\n",
    "env.visual_board"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['[S]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[F]', '[ ]', '[ ]']]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The agent\n",
    "- The agent is a chess Piece (king, queen, rook, knight or bishop)\n",
    "- The agent has a behavior policy determining what the agent does in what state"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "p = Piece(piece='king')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reinforce\n",
    "- The reinforce object contains the algorithms for solving move chess\n",
    "- The agent and the environment are attributes of the Reinforce object"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "r = Reinforce(p,env)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.1 Monte Carlo Control"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Implementation**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(inspect.getsource(r.monte_carlo_learning))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    def monte_carlo_learning(self, epsilon=0.1):\n",
      "        \"\"\"\n",
      "        Learn move chess through monte carlo control\n",
      "        :param epsilon: exploration rate\n",
      "        :return:\n",
      "        \"\"\"\n",
      "        state = (0, 0)\n",
      "        self.env.state = state\n",
      "\n",
      "        # Play out an episode\n",
      "        states, actions, rewards = self.play_episode(state, epsilon=epsilon)\n",
      "\n",
      "        first_visits = []\n",
      "        for idx, state in enumerate(states):\n",
      "            action_index = actions[idx]\n",
      "            if (state, action_index) in first_visits:\n",
      "                continue\n",
      "            r = np.sum(rewards[idx:])\n",
      "            if (state, action_index) in self.agent.Returns.keys():\n",
      "                self.agent.Returns[(state, action_index)].append(r)\n",
      "            else:\n",
      "                self.agent.Returns[(state, action_index)] = [r]\n",
      "            self.agent.action_function[state[0], state[1], action_index] = \\\n",
      "                np.mean(self.agent.Returns[(state, action_index)])\n",
      "            first_visits.append((state, action_index))\n",
      "        # Update the policy. In Monte Carlo Control, this is greedy behavior with respect to the action function\n",
      "        self.agent.policy = self.agent.action_function.copy()\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Demo**  \n",
    "We do 100 iterations of monte carlo learning while maintaining a high exploration rate of 0.5:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "for k in range(100):\r\n",
    "    eps = 0.5\r\n",
    "    r.monte_carlo_learning(epsilon=eps)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "r.visualize_policy()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['↙', '↖', '↓', '←', '↗', '→', '→', '↗'],\n",
      " ['←', '↓', '↙', '↓', '↖', '←', '↘', '↗'],\n",
      " ['↑', '↙', '↖', '↙', '→', '↖', '→', '→'],\n",
      " ['→', '↙', '↙', '↗', '←', '→', '↙', '↘'],\n",
      " ['↓', '↙', '↓', '←', '↙', '↖', '↗', '↑'],\n",
      " ['↓', '↘', '→', '→', '↙', '↙', '↘', '↗'],\n",
      " ['↗', '↓', '↘', '↖', '↘', '↓', '↙', '←'],\n",
      " ['↗', '→', '↖', '→', '→', 'F', '↗', '←']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Best action value for each state:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "r.agent.action_function.max(axis=2).astype(int)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ -52,  -54,  -51,  -55,  -98,  -76,  -70,  -63],\n",
       "       [ -47,  -47,  -47,  -56,  -77, -140,  -81,  -58],\n",
       "       [ -57,  -39,  -44,  -43,  -85,  -65, -170, -174],\n",
       "       [ -32,  -31,  -32,  -59,  -66, -105,  -63,  -84],\n",
       "       [ -30,  -34,  -37,  -36,  -39,  -62, -146,  -44],\n",
       "       [ -26,  -16,  -25,  -24,  -31,  -57,  -65,  -22],\n",
       "       [ -23,  -32,  -12,  -24,   -1,   -1,   -1,   -3],\n",
       "       [ -35,  -30,  -32,   -4,   -1,    0,    0,   -2]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.2 Temporal Difference Learning "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Theory**\n",
    "* Like Policy Iteration, we can back up state-action values from the successor state action without waiting for the episode to end. \n",
    "* We update our state-action value in the direction of the successor state action value.\n",
    "* The algorithm is called SARSA: State-Action-Reward-State-Action.\n",
    "* Epsilon is gradually lowered (the GLIE property)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Implementation**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(inspect.getsource(r.sarsa_td))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    def sarsa_td(self, n_episodes=1000, alpha=0.01, gamma=0.9):\n",
      "        \"\"\"\n",
      "        Run the sarsa control algorithm (TD0), finding the optimal policy and action function\n",
      "        :param n_episodes: int, amount of episodes to train\n",
      "        :param alpha: learning rate\n",
      "        :param gamma: discount factor of future rewards\n",
      "        :return: finds the optimal policy for move chess\n",
      "        \"\"\"\n",
      "        for k in range(n_episodes):\n",
      "            state = (0, 0)\n",
      "            self.env.state = state\n",
      "            episode_end = False\n",
      "            epsilon = max(1 / (1 + k), 0.05)\n",
      "            while not episode_end:\n",
      "                state = self.env.state\n",
      "                action_index = self.agent.apply_policy(state, epsilon)\n",
      "                action = self.agent.action_space[action_index]\n",
      "                reward, episode_end = self.env.step(action)\n",
      "                successor_state = self.env.state\n",
      "                successor_action_index = self.agent.apply_policy(successor_state, epsilon)\n",
      "\n",
      "                action_value = self.agent.action_function[state[0], state[1], action_index]\n",
      "                successor_action_value = self.agent.action_function[successor_state[0],\n",
      "                                                                    successor_state[1], successor_action_index]\n",
      "\n",
      "                q_update = alpha * (reward + gamma * successor_action_value - action_value)\n",
      "\n",
      "                self.agent.action_function[state[0], state[1], action_index] += q_update\n",
      "                self.agent.policy = self.agent.action_function.copy()\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Demonstration**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "p = Piece(piece='king')\r\n",
    "env = Board()\r\n",
    "r = Reinforce(p,env)\r\n",
    "r.sarsa_td(n_episodes=10000,alpha=0.2,gamma=0.9)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "r.visualize_policy()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['↘', '↘', '↘', '→', '↘', '→', '↑', '↗'],\n",
      " ['↘', '↙', '↘', '↙', '→', '↘', '→', '↓'],\n",
      " ['↘', '↘', '↓', '↓', '↙', '↖', '↙', '↖'],\n",
      " ['↘', '↘', '↓', '↙', '↘', '↘', '↑', '↙'],\n",
      " ['→', '↘', '↘', '↘', '↘', '↓', '↘', '↙'],\n",
      " ['↘', '→', '↘', '↘', '↘', '↘', '↓', '↙'],\n",
      " ['←', '↗', '↘', '↘', '↘', '↓', '↙', '←'],\n",
      " ['↗', '→', '→', '→', '→', 'F', '←', '←']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.3 TD-lambda\n",
    "**Theory**  \n",
    "In Monte Carlo we do a full-depth backup while in Temporal Difference Learning we de a 1-step backup. You could also choose a depth in-between: backup by n steps. But what value to choose for n?\n",
    "* TD lambda uses all n-steps and discounts them with factor lambda\n",
    "* This is called lambda-returns\n",
    "* TD-lambda uses an eligibility-trace to keep track of the previously encountered states\n",
    "* This way action-values can be updated in retrospect"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Implementation**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "print(inspect.getsource(r.sarsa_lambda))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    def sarsa_lambda(self, n_episodes=1000, alpha=0.05, gamma=0.9, lamb=0.8):\n",
      "        \"\"\"\n",
      "        Run the sarsa control algorithm (TD lambda), finding the optimal policy and action function\n",
      "        :param n_episodes: int, amount of episodes to train\n",
      "        :param alpha: learning rate\n",
      "        :param gamma: discount factor of future rewards\n",
      "        :param lamb: lambda parameter describing the decay over n-step returns\n",
      "        :return: finds the optimal move chess policy\n",
      "        \"\"\"\n",
      "        for k in range(n_episodes):\n",
      "            self.agent.E = np.zeros(shape=self.agent.action_function.shape)\n",
      "            state = (0, 0)\n",
      "            self.env.state = state\n",
      "            episode_end = False\n",
      "            epsilon = max(1 / (1 + k), 0.2)\n",
      "            action_index = self.agent.apply_policy(state, epsilon)\n",
      "            action = self.agent.action_space[action_index]\n",
      "            while not episode_end:\n",
      "                reward, episode_end = self.env.step(action)\n",
      "                successor_state = self.env.state\n",
      "                successor_action_index = self.agent.apply_policy(successor_state, epsilon)\n",
      "\n",
      "                action_value = self.agent.action_function[state[0], state[1], action_index]\n",
      "                if not episode_end:\n",
      "                    successor_action_value = self.agent.action_function[successor_state[0],\n",
      "                                                                        successor_state[1], successor_action_index]\n",
      "                else:\n",
      "                    successor_action_value = 0\n",
      "                delta = reward + gamma * successor_action_value - action_value\n",
      "                self.agent.E[state[0], state[1], action_index] += 1\n",
      "                self.agent.action_function = self.agent.action_function + alpha * delta * self.agent.E\n",
      "                self.agent.E = gamma * lamb * self.agent.E\n",
      "                state = successor_state\n",
      "                action = self.agent.action_space[successor_action_index]\n",
      "                action_index = successor_action_index\n",
      "                self.agent.policy = self.agent.action_function.copy()\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Demonstration**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "p = Piece(piece='king')\r\n",
    "env = Board()\r\n",
    "r = Reinforce(p,env)\r\n",
    "r.sarsa_lambda(n_episodes=10000,alpha=0.2,gamma=0.9)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "r.visualize_policy()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['↓', '↘', '↘', '↓', '↓', '↙', '↓', '→'],\n",
      " ['↘', '↘', '↘', '↓', '↙', '↑', '→', '↙'],\n",
      " ['↘', '↓', '↘', '↘', '↘', '↓', '↙', '↓'],\n",
      " ['→', '↘', '↘', '↓', '↓', '↓', '←', '↙'],\n",
      " ['↘', '↘', '↘', '↘', '↓', '↘', '↓', '↙'],\n",
      " ['↘', '↘', '↘', '↘', '↓', '↓', '↙', '↙'],\n",
      " ['↘', '→', '↘', '↘', '↘', '↓', '↙', '↙'],\n",
      " ['→', '→', '↗', '→', '→', 'F', '←', '←']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.4 Q-learning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Theory**\n",
    "* In SARSA/TD0, we back-up our action values with the succesor action value\n",
    "* In SARSA-max/Q learning, we back-up using the maximum action value. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Implementation**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "print(inspect.getsource(r.sarsa_lambda))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    def sarsa_lambda(self, n_episodes=1000, alpha=0.05, gamma=0.9, lamb=0.8):\n",
      "        \"\"\"\n",
      "        Run the sarsa control algorithm (TD lambda), finding the optimal policy and action function\n",
      "        :param n_episodes: int, amount of episodes to train\n",
      "        :param alpha: learning rate\n",
      "        :param gamma: discount factor of future rewards\n",
      "        :param lamb: lambda parameter describing the decay over n-step returns\n",
      "        :return: finds the optimal move chess policy\n",
      "        \"\"\"\n",
      "        for k in range(n_episodes):\n",
      "            self.agent.E = np.zeros(shape=self.agent.action_function.shape)\n",
      "            state = (0, 0)\n",
      "            self.env.state = state\n",
      "            episode_end = False\n",
      "            epsilon = max(1 / (1 + k), 0.2)\n",
      "            action_index = self.agent.apply_policy(state, epsilon)\n",
      "            action = self.agent.action_space[action_index]\n",
      "            while not episode_end:\n",
      "                reward, episode_end = self.env.step(action)\n",
      "                successor_state = self.env.state\n",
      "                successor_action_index = self.agent.apply_policy(successor_state, epsilon)\n",
      "\n",
      "                action_value = self.agent.action_function[state[0], state[1], action_index]\n",
      "                if not episode_end:\n",
      "                    successor_action_value = self.agent.action_function[successor_state[0],\n",
      "                                                                        successor_state[1], successor_action_index]\n",
      "                else:\n",
      "                    successor_action_value = 0\n",
      "                delta = reward + gamma * successor_action_value - action_value\n",
      "                self.agent.E[state[0], state[1], action_index] += 1\n",
      "                self.agent.action_function = self.agent.action_function + alpha * delta * self.agent.E\n",
      "                self.agent.E = gamma * lamb * self.agent.E\n",
      "                state = successor_state\n",
      "                action = self.agent.action_space[successor_action_index]\n",
      "                action_index = successor_action_index\n",
      "                self.agent.policy = self.agent.action_function.copy()\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Demonstration**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "p = Piece(piece='king')\r\n",
    "env = Board()\r\n",
    "r = Reinforce(p,env)\r\n",
    "r.q_learning(n_episodes=1000,alpha=0.2,gamma=0.9)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "r.visualize_policy()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['↘', '↑', '↖', '→', '→', '↗', '↑', '↖'],\n",
      " ['↘', '↘', '↘', '↑', '↗', '↘', '↘', '↗'],\n",
      " ['↘', '↓', '↘', '↘', '↘', '↖', '↖', '↘'],\n",
      " ['→', '↘', '↓', '↘', '↙', '↙', '↗', '↖'],\n",
      " ['↖', '↘', '↘', '↘', '↙', '↓', '↑', '↓'],\n",
      " ['↘', '↘', '→', '↘', '↓', '↙', '↙', '↙'],\n",
      " ['↖', '↙', '←', '↘', '↘', '↓', '↙', '←'],\n",
      " ['↓', '→', '→', '↗', '→', 'F', '←', '↖']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "r.agent.action_function.max(axis=2).round().astype(int)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-5, -5, -5, -4, -4, -4, -4, -3],\n",
       "       [-5, -5, -4, -4, -4, -4, -3, -3],\n",
       "       [-4, -4, -4, -4, -4, -3, -3, -3],\n",
       "       [-4, -3, -3, -3, -3, -3, -3, -3],\n",
       "       [-3, -3, -3, -3, -3, -3, -3, -3],\n",
       "       [-3, -3, -3, -2, -2, -2, -2, -2],\n",
       "       [-3, -3, -2, -2, -1, -1, -1, -2],\n",
       "       [-3, -3, -2, -2, -1,  0, -1, -1]])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, in a basic approach, that's temporal difference learning. The basic idea is that we establish an initial estimate, then explore a space and update our previous estimate based on our findings. "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}