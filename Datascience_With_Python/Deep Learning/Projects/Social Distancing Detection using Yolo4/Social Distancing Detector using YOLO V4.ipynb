{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f06875",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf6718b",
   "metadata": {},
   "source": [
    "This project based on Social Distancing detection uses computer vision and deep-learning to understand various aspects of the images or videos based on frames that would be provided as an input to the algorithms.\n",
    "\n",
    "Project Structure -\n",
    "* Firstly, import the Libraries \n",
    "* Secondly, write basic config files and specify the input, output files\n",
    "* Then, make a ```detector``` function which can detect people in a still image\n",
    "* Next, import the YOLO V4 config files and weights from the directory\n",
    "* Finally, parse the video input one frame by one into the ```detector``` function and get the bounding boxes as output to write in output video file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e1b80",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc5f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as dist\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c47d7",
   "metadata": {},
   "source": [
    "## Defining the Config variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "777c4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base path to YOLO directory\n",
    "MODEL_PATH = \"yolo-coco\"\n",
    "# initialize minimum probability to filter weak detections along with\n",
    "# the threshold when applying non-maxima suppression\n",
    "MIN_CONF = 0.3\n",
    "NMS_THRESH = 0.3\n",
    "USE_GPU = False\n",
    "# define the minimum safe distance (in pixels) that two people can be from each other\n",
    "MIN_DISTANCE = 50\n",
    "\n",
    "# load the input files\n",
    "input = \"pedestrians.mp4\" # change this to your custom video file name/path\n",
    "output = \"output.avi\" # output is updated into this file name\n",
    "display = 1    # change this if you don't want to display the video output while running of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d23f0",
   "metadata": {},
   "source": [
    "## Detecting people in a single frame/image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db886a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_people(frame, net, ln, personIdx=0):\n",
    "\t# grab the dimensions of the frame and  initialize the list of\n",
    "\t# results\n",
    "\t(H, W) = frame.shape[:2]\n",
    "\tresults = []\n",
    "\n",
    "    # construct a blob from the input frame and then perform a forward\n",
    "\t# pass of the YOLO object detector, giving us our bounding boxes\n",
    "\t# and associated probabilities\n",
    "\tblob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),\n",
    "\t\tswapRB=True, crop=False)\n",
    "\tnet.setInput(blob)\n",
    "\tlayerOutputs = net.forward(ln)\n",
    "\t# initialize our lists of detected bounding boxes, centroids, and\n",
    "\t# confidences, respectively\n",
    "\tboxes = []\n",
    "\tcentroids = []\n",
    "\tconfidences = []\n",
    "\tfor output in layerOutputs:\n",
    "\t\t# loop over each of the detections\n",
    "\t\tfor detection in output:\n",
    "\t\t\tscores = detection[5:]\n",
    "\t\t\tclassID = np.argmax(scores)\n",
    "\t\t\tconfidence = scores[classID]\n",
    "\t\t\t# filter detections by (1) ensuring that the object detected was a person and (2) that the minimum confidence is met\n",
    "\t\t\tif classID == personIdx and confidence > MIN_CONF:\n",
    "\t\t\t\t# scale the bounding box coordinates back relative to the size of the image, keeping in mind that YOLO\n",
    "\t\t\t\t# actually returns the center (x, y)-coordinates of the bounding box followed by the boxes' width and height\n",
    "\t\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
    "\t\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\t\t\t\tx = int(centerX - (width / 2))\n",
    "\t\t\t\ty = int(centerY - (height / 2))\n",
    "\t\t\t\t# update our list of bounding box coordinates,\n",
    "\t\t\t\t# centroids, and confidences\n",
    "\t\t\t\tboxes.append([x, y, int(width), int(height)])\n",
    "\t\t\t\tcentroids.append((centerX, centerY))\n",
    "\t\t\t\tconfidences.append(float(confidence))\n",
    "    \t# apply non-maxima suppression to suppress weak, overlapping bounding boxes\n",
    "\tidxs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)\n",
    "\t# ensure at least one detection exists\n",
    "\tif len(idxs) > 0:\n",
    "\t\t# loop over the indexes we are keeping\n",
    "\t\tfor i in idxs.flatten():\n",
    "\t\t\t# extract the bounding box coordinates\n",
    "\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
    "\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
    "\t\t\tr = (confidences[i], (x, y, x + w, y + h), centroids[i])\n",
    "\t\t\tresults.append(r)\n",
    "\t# return the list of results\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca459d6f",
   "metadata": {},
   "source": [
    "## Loading YOLO V4 and COCO names from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b82554e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading YOLO from disk...\n",
      "[INFO] accessing video stream...\n"
     ]
    }
   ],
   "source": [
    "# load the COCO class labels our YOLO model was trained on\n",
    "labelsPath = os.path.sep.join([MODEL_PATH, \"coco.names\"])\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "# derive the paths to the YOLO weights and model configuration\n",
    "weightsPath = os.path.sep.join([MODEL_PATH, \"yolov4.weights\"])\n",
    "configPath = os.path.sep.join([MODEL_PATH, \"yolov4.cfg\"])\n",
    "# load our YOLO object detector trained on COCO dataset (80 classes)\n",
    "print(\"[INFO] loading YOLO from disk...\")\n",
    "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
    "# check if we are going to use GPU\n",
    "if USE_GPU:\n",
    "\t# set CUDA as the preferable backend and target\n",
    "\tprint(\"[INFO] setting preferable backend and target to CUDA...\")\n",
    "\tnet.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "\tnet.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "    # determine only the *output* layer names that we need from YOLO\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "# initialize the video stream and pointer to output video file\n",
    "print(\"[INFO] accessing video stream...\")\n",
    "vs = cv2.VideoCapture(input)\n",
    "writer = None\n",
    "# loop over the frames from the video stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898faacf",
   "metadata": {},
   "source": [
    "## Final Step\n",
    "Here, parse the input video into the detector() function and write the bounding boxes and No. of people violating Social Distancing into the output.avi video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d660e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\t# read the next frame from the file\n",
    "\t(grabbed, frame) = vs.read()\n",
    "\t# if the frame was not grabbed, then we have reached the end\n",
    "\t# of the stream\n",
    "\tif not grabbed:\n",
    "\t\tbreak\n",
    "\t# resize the frame and then detect people (and only people) in it\n",
    "\tframe = imutils.resize(frame, width=700)\n",
    "\tresults = detect_people(frame, net, ln,\n",
    "\t\tpersonIdx=LABELS.index(\"person\"))\n",
    "\tviolate = set()\n",
    "    # ensure there are *at least* two people detections \n",
    "\tif len(results) >= 2:\n",
    "\t\t# extract all centroids from the results and compute the\n",
    "\t\t# Euclidean distances between all pairs of the centroids\n",
    "\t\tcentroids = np.array([r[2] for r in results])\n",
    "\t\tD = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
    "\t\tfor i in range(0, D.shape[0]):\n",
    "\t\t\tfor j in range(i + 1, D.shape[1]):\n",
    "\t\t\t\t# check to see if the distance between any two\n",
    "\t\t\t\t# centroid pairs is less than the configured number\n",
    "\t\t\t\t# of pixels\n",
    "\t\t\t\tif D[i, j] < MIN_DISTANCE:\n",
    "\t\t\t\t\t# update our violation set with the indexes of\n",
    "\t\t\t\t\t# the centroid pairs\n",
    "\t\t\t\t\tviolate.add(i)\n",
    "\t\t\t\t\tviolate.add(j)\n",
    "    \t# loop over the results\n",
    "\tfor (i, (prob, bbox, centroid)) in enumerate(results):\n",
    "\t\t# extract the bounding box and centroid coordinates, then\n",
    "\t\t# initialize the color of the annotation\n",
    "\t\t(startX, startY, endX, endY) = bbox\n",
    "\t\t(cX, cY) = centroid\n",
    "\t\tcolor = (0, 255, 0)\n",
    "\t\t# if the index pair exists within the violation set, then\n",
    "\t\t# update the color\n",
    "\t\tif i in violate:\n",
    "\t\t\tcolor = (0, 0, 255)\n",
    "\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "\t\tcv2.circle(frame, (cX, cY), 5, color, 1)\n",
    "\t# draw the total number of social distancing violations on the\n",
    "\t# output frame\n",
    "\ttext = \"People NOT following Social Distancing: {}\".format(len(violate))\n",
    "\tcv2.putText(frame, text, (10, frame.shape[0] - 25),\n",
    "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 0, 255), 3)\n",
    "    # check to see if the output frame should be displayed to our\n",
    "\t# screen\n",
    "\tif display > 0:\n",
    "\t\t# show the output frame\n",
    "\t\tcv2.imshow(\"Frame\", frame)\n",
    "\t\tkey = cv2.waitKey(1) & 0xFF\n",
    "\t\t# if the `q` key was pressed, break from the loop\n",
    "\t\tif key == ord(\"q\"):\n",
    "\t\t\tbreak\n",
    "\t# if an output video file path has been supplied and the video\n",
    "\t# writer has not been initialized, do so now\n",
    "\tif output != \"\" and writer is None:\n",
    "\t\t# initialize our video writer\n",
    "\t\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "\t\twriter = cv2.VideoWriter(output, fourcc, 25,\n",
    "\t\t\t(frame.shape[1], frame.shape[0]), True)\n",
    "\t# if the video writer is not None, write the frame to the output video file\n",
    "\tif writer is not None:\n",
    "\t\twriter.write(frame)\n",
    "cv2.destroyAllWindows()\n",
    "vs.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc3a419",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The Social Distancing Detector was built. YOLO V4 model and OpenCV was used for object detection, people tracking and bounding box creation. In the output video generated, number of people violating Social Distancing can be seen too, that too in realtime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
