## Dimensionality Reduction

## [Click here to watch video](https://drive.google.com/file/d/1BmFveqpObysMwgxcj_tDiuRLhclgNSoc/view?usp=sharing)
- It is a unsupervised machine learning model.
- It is used to convert high dimentional dataset to low dimentional dataset.
- It is used to convert correlated features/variables to uncorrelated features/variables.

**Types of Dimensionality Reduction:**
- Principal Component Analysis (PCA)
- Singular Value Decomposition (SVD)
- Linear Discriminant Analysis  (LDA)

**Principal Component Analysis (PCA):**
- PCA is a statistical method that is used to convert a set of correlated variables to a set of uncorrelated variables.
- It is a unsupervised machine learning model.
- Broadly, used as a EDA(Exploratory Data Analysis) method.
- Used to convert large number of features into a fewer features with some loss of information:
   - Identify the relationship between columns
   - Visualize the multivariate data using 2 Principal Components
   - It will help condense the features 

![2](https://user-images.githubusercontent.com/79050917/137140916-b87cedc1-f547-49fd-abe5-aa6af27f278d.PNG)


**Singular Value Decomposition (SVD):**
- The Singular Value Decomposition (SVD), a method from linear algebra that has been generally used as a dimensionality reduction technique in machine learning. SVD is a matrix factorisation technique, which reduces the number of features of a dataset

**Linear Discriminant Analysis:** 
- Linear Discriminant Analysis or Normal Discriminant Analysis or Discriminant Function Analysis is a dimensionality reduction technique that is commonly used for supervised classification problems. It is used for modeling differences in groups i.e. separating two or more classes. It is used to project the features in higher dimension space into a lower dimension space.

![3](https://user-images.githubusercontent.com/79050917/137140893-4d13f6d1-26d9-4658-b218-34141c772d29.PNG)


**When/Why to use:**
- It is technique that is particularly useful in processing data where multi-colinearity exists between the features/variables.
- It  can be used when the dimensions of the input features are high (e.g. a lot of variables).
- It  can be also used for denoising and data compression.

**Some of the Applications:**
- Computation is fast
- Image compression

![1](https://user-images.githubusercontent.com/79050917/137138922-443e5af1-6234-461c-9636-1d9240e7d947.PNG)

