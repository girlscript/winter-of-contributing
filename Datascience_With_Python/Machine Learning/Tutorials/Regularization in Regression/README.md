# **Regularization in Regression**

##  **Introduction**

**What is Regularization ?**


In mathematics, statistics, finance, computer science, particularly in machine learning and inverse problems, regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting.

## **Explanation**

**Why do we need Regularization ?**

Regularization is used in machine learning models to cope with the problem of overfitting i.e. when the difference between training error and the test error is too high.

**Regularization in Regression**
There are mainly two types of regularization techniques:


- Lasso Regression
- Ridge Regression

**Lasso Regression**
A regression model which uses L1 Regularization technique is called LASSO(Least Absolute Shrinkage and Selection Operator) regression.
**Ridge Regression**
A regression model that uses L2 regularization technique is called Ridge regression. 
Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function(L)


- For the Dataset being used [Click here](https://www.kaggle.com/quantbruce/real-estate-price-prediction).

## **Visual Representation**

![]()

## **Libraries Used**

- pandas
- sklearn

## **Usage**

- Mainly used to deal with overfitting of data.
- Improves score for training and testing data.

## **Useful Resources to check out:**

- https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a
- https://www.geeksforgeeks.org/regularization-in-machine-learning/
- https://www.analyticsvidhya.com/blog/2021/05/complete-guide-to-regularization-techniques-in-machine-learning/

