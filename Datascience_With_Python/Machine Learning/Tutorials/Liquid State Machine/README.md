I’ve completed the documentation of the Liquid State machine that comprises of<BR/><BR/>
<ul>
  <li><b>WHAT IS A LIQUID STATE MACHINE(LSM)?</b><br/><br/>
Over the past decades, artificial neural algorithms have developed to an extent that they can perform more human brain like functions.
   Recurrent  Neural Networks (RNNs) and their variants similar as Long Short Term Memory (LSTM) networks have come the state-of-the- art for recycling spatio-temporal data.
    Such efficient human brain like capabilities are attained at the cost of increased structural and training complexity, 
    and therefore significant power consumption, storehouse conditions, and detention. 
 
 In this work, I have covered a particular type of spiking RNN; the Liquid State Machine (LSM).
    A Liquid State Machines (LSM) is a computational model which consists basically of intermittent and arbitrary spiking neural networks and multiple neurons.
  An LSM consists of a large collection of units where each unit receives time-varying input 
  from external sources as well as from other nodes. All the nodes are randomly connected to each other.
  The intermittent nature of the connections turns the time-varying input into a Spatio-temporal pattern of activations in the network.
  </li>
  <br/><br/>
  
  <li><b>EMERGE OF LIQUID STATE MACHINE</b><br/><br/>
    The conception of LSM was motivated by the versatility nature of the neocortex. The neocortex is a crucial portion of the mammalian brain and through the networks of neurons, it controls functions similar to sensory perception, motor command generation, spatial logic and consciousness allowed amongst others. The functionality of the neocortex is grounded on the conformation of six connected layers of neurons that are generated successionally over a long time period.
<br/>

 To understand what a liquid state machine is, it's important to understand the type of machine learning program into which it falls. These types of machine learning are sometimes called “third-generation” neural networks or spiking neural networks. The spiking neural network, which utilizes numerous of the same models as a liquid state machine, adds a property of time to synaptic and neural rudiments. 
 In a liquid state machine model, evaluation of spiking neural exertion leads to a specific pattern of neuron network activation. So certain types of memory are saved throughout the process. 
 Another indication of the nature of a liquid state machine has to do with the name of this particular kind of spiking neural network. 
The idea is that dropping a gravestone or other solid item into a body of water or some other liquid produces ripples on the face, and exertion under the face, that can be estimated to understand what's passing in the system. In the same way, humans can estimate the operations of a liquid state machine to understand further how it's modelling mortal brain exertion. Still, an important thing to note is that liquid state machines have some particular challenges. One of these is that it becomes too delicate to observe computational work and insolvable to reverse the system because there are less strict rules on the process itself. Experts point out that in a liquid state machine, circuits aren't hardcoded to do specific tasks, and because of the versatility of the system and its design, there's lower control over the neural network process in general.  
  </li>
  <br/><br/>
  
  <li><b>WHAT IS A SPIKING NEURAL NETWORK(SNN)?</b><br/><br/>
Spiking Neural Network (SNN) is a third-generation artificial neuron that's utmost biologically- inspired. 
  They're preferred above the former generations of artificial neurons because the spiking neurons operate in the temporal sphere and 
  their calculation is grounded on time resources. SNNs are getting a dominant agent for brain-inspired neuromorphic 
  computing- emulating the brain with computational tackle. The choice of SNNs is due to their essential effectiveness
  and delicacy on multitudinous cognitive tasks that include speech recognition and image bracket among others.
  </li>
<br/><br/>
  <li><b>PROPERTIES OF LSM's</b> <br/><br/>
 There are two properties of Liquid State Machine that are named as <br/><br/>
   <ul>
    <li>Separation Property (SP) and,</li>
     <li>Approximation Property (AP)</li>
    </ul>
      <br/><br/> These can be used to quantify a liquid's capability to give an advanced projection of the input data. 
    With respect to application, SP gives a measure of the liquid's capability to separate input cases that belong to different classes. AP, 
    on the other hand, gives a measure of the closeness of the liquid's representation of two inputs that belong to the same class. <br/><br/>
 Several methods of quantifying the SP and AP as a measure of the computational power of an LSM are known. Two styles of measuring the SP are pairwise separation property and direct separation property. The pairwise separation property is the distance between two simultaneous time states of the liquid, performing from two different input spike train. The pairwise separation property can be used as a measure of the kernel quality for two given inputs. 
    Still, utmost real- life operations deal with further than two input spike trains.     
  </li>
  <br/><br/>
  <li><b>DIFFERENCE BETWEEN LIQUID STATE MACHINE(LSM) & ECHO STATE NETWORK(ESM)?</b><br/><br/>
Well, there are not many differences between both of them, b
    oth are recurrent neural networks where the inputs and outputs are trained but recurrent connections are not trained. 
    However, the one key difference among them is that. Echo state networks are a bit more mainstream 
    and generally implemented as artificial recurrent networks with random dynamics, subject to some constraints such as sparsity. 
    Liquid state machines are a bit more niche and are sometimes implemented as spiking networks with more versatile dynamics.
  </li>
  <br/><br/>
  <li><b>HOW LSM IS IMPORTANT IN MACHINE LEARNING?</b><br/><br/>
<ul>
  <li>LSMs have been put forward as a way to explain the operation of the brain.</li> 
   <li> LSMs proved to be an enhancement over the proposition of artificial neural networks because 
     Circuits aren't hard wired to perform a specific task. </li>
  <li>Simultaneous time inputs are handled naturally. </li>
  <li>Calculations on various time scales can be done using the same network.</li> 
   <li>The same network can perform multiple calculations. </li>
    </ul>
  </li>
  <br/><br/>

<small>References: https://www.scholarpedia.org<br>
https://www.researchgate.net
  </small>
