{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3PvHRYUXrEj"
   },
   "source": [
    "# **Hyperparameter Tuning**\n",
    "#### In this notebook, we will be implementing hyperparameter tuning and comparing results of two models that will be obtaining after tweaking certain hyperparameters. We will be experimenting with hyperparameters such as batch size and number of neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXTw8ZHcxnF9"
   },
   "source": [
    "# 1. Learning rate α\n",
    "#### Our main motto while administering learning rate is to minimize loss function. It is advised first to start to with learning_rate = $10^{2}$ and then go with $10^{3}$, $10^{4}$ and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNf9sSqeXZXI"
   },
   "outputs": [],
   "source": [
    "#'lr' has now been changed to 'learning_rate'\n",
    "optimizer = tf.keras.optimizers.Adam.(learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIoiz9Z_TTWP"
   },
   "source": [
    "#### An example for exponential decay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sQvVBzCTYki"
   },
   "outputs": [],
   "source": [
    "lr_initial = 0.01\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(lr_initial,\n",
    "                                                             decay_steps = 10000,\n",
    "                                                             decay_rate = 0.99,\n",
    "                                                             staircase = True)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate = lr_schedule),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_EOOxAsSHeQ"
   },
   "source": [
    "## 2. Momentum β\n",
    "#### Usually β is kept between 0.5 and 0.99, which helps you to tweak the gradient descent as such. Momentum acts like a boost or acceleration for many functions like RMSprop, Adagrad etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqw30TnhXJzS"
   },
   "source": [
    "## 3. Number of hidden layers\n",
    "#### This hyperparameter varies from model to model. To many hidden layers and hidden units in the same might cause the model to overfit. In the same way, more convoluted hidden layers and fully connected layers improve model's performance. Using few too hidden layers will cause underfitting. Make sure you tweak your model with appropriate number of layers considering constraints like how major work is the model working on, number of training images etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzzAJ0rCZOsF"
   },
   "source": [
    "## 4. Mini-batch size\n",
    "#### Batch sizes in powers of 2 are most common. That is 4, 8, 16, 32, 64 and so on. The general rule is to take values 32 onwards as smaller batch size were seen to be less efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGWaNnNFW3ZS"
   },
   "outputs": [],
   "source": [
    "#code snippet where batch_size comes into picture \n",
    "batch_size = 64;\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=batch_size, callbacks=[print_weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBPt1-cwcdAX"
   },
   "source": [
    "## 5. Learning rate decay\n",
    "#### Learning decay essentially means starting with a particular value of learning_rate and slowly decaying it or reducing it until we reach local minima. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3bsMTyZiH91"
   },
   "source": [
    "## 6. Activation function\n",
    "#### ***Sigmoid***: The non linear activation fucntion called sigmoid or logistic fucntion is popularly used, as its probability output lies between 0 and 1. \n",
    "#### $$ h_\\theta (x)=\\frac{\\mathrm{1} }{\\mathrm{1} + e^-\\theta^Tx }  $$ \n",
    "\n",
    "#### ***ReLU***: Rectified Linear Unit has a good advantage when used as its derivative is more faster to compute than the sigmoid function. It also has reduced likelihood of the gradient to vanish. \n",
    "\n",
    "![relu.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYwAAAD8CAMAAABTsmMNAAAAQlBMVEX////19fl/f39HR0cgICAKCgpoaP+/v78yMjIMDP1jY2Po6Oza2tqTk5OxsbKgoKG5ubrQ0P8sLP9LS/qurv+Jif2MEhelAAALrUlEQVR42uydjW6jOhCFx/YM+I/AtuX9X/V6oJcuEbrpbpowujlHWm2afk2lfjJwwBhCEARBEARBEARBdsmejhJ7+pv4TIiGRTi77cueSRPG7XWLk0BbxvgnejRBWo416e9AtvBEJfr/ljGKlHtkjNQCGd+SQT4QOc+c3bGM4ONARCWI5CotlD3xhYikUmAJlQYRCfphLn9+ziA8/i6jyvpyjF4aRyWzBCctxWeiS9SPIR6iZEcvmmVkZKIcigv+UEaVOkQiitm5fhkZKmP5CzZidC5HfWf9MB+LjrRevLtIOZIhIw1MFHJx/fo7fKYqk/PsiGMpPNCLhkUkFCKpRD0fyvCRilTqhVo2GVWc/qcp4jYZOmAmXmHut31G/JLBRE5KWUxtMnxYeR5feX/OE/VcqciSTcZARBN/IgNRyHSJOxkUR6cGveqsmwx9q35+Dk+HmylS9WrrS8b643Eknl5cBvlAJIU0PX/tjoewviXCLHI1MvTb+nJcTNarkXEgQ4cP8SbjeGRAxvrHDIWKjhLXQpP0VHlQQL9TSpULRa/7jIndKqNIaMAQnctSyefdPuNaxjrARtlkfO4zqrhtnzGwgwyiHMh5Fh6oF02lMQp70rimYUVqEMnkgsgig4IU0i95lEo17o6mDmRMLD5sMtajKaL879EUS6j04jIQBEEQBEH+95G4C8fjgPoeJnRPIu3S03FAfQ+LkAEZoCDDFnWHjCwNLoFDgYzTZfS1wd6T95BxugxSGVyoMGT8DDVP98kQ0n9Lhhi53+XSHwfUYd7T3O/yVzIwMn6CmlOHzZQR6iN1dIcM7MB/jFIX7+4OGZmFRxza/gz1K705lD4bVEmpoIHboNxb+oXTITYo954+cG7KCNWlD5woNEJ1acZZWyPUnDqcQjdCtYKB6xlGKC0YkGGDai4KrvTZoLRg4LKrDUpd4Bq4EaqVPUxIMEJ1acbsECPUnDpM1TFCtYKBeVNGKC0YkGGDUheYUWiDKu2gFtM7bVBaMDDX1gjVCgYmPhuhujRjFroRak4zbgkwQn2kDvdnGKF0uhpk2KC0YODOJRtU0atJD5MxxJgdZNygdgXjYTIKOwojZNymtulqj5RRKEyQ8T1qdfHAzZRIxj7jNrWVvUeOjFBcGHEb2XeoLr3/0Yf9sYxLJhozRsZtSgvGg2/K76OjPEDGbUoLxoNlkGcc2t6mVhdYrsIGpQUDa4fYoNQFFnIxQrWCgVV1jFDvacYSR0aoOXVYb8oI1QoGFv8yQjUXE2TYoLRgYFk8G1R5SwVrFNqgtGBgwUgblF5NwuqdRqgufWApVSPUnGasa2uEmlOHRYaNUK1gYMVnI5QWDMiwQTUXBWuh26C0YGBhehuUusBTAoxQrezhkQ1GqC7NeH6GEWpOHR5mYoRqBQNPljFCacGADBuUusAzl2xQuvgBHoBlg3Lq4iwZJTD3kLErGKfJyCO5Ahm7gnGWDMfYTF1dTTpNhqsxx+xw59KSaU7vN6iH3rnUS0/ZY2TsFj84azNVmKgPkLEVjDNlUKzkMTK2Z/U8TUZ11A/l+s0YA46mtoLxNBmRKvuA0ndIOS0YT5UxDBQh45Dq1MUzZYyxQsYxpWXvqTJqHql6yDig5tThSflGKL2a9FQZgaIGm6k9tRWMp8ooVJdAxp7apqs9eZ9BLT1k7KitYDxZRvTkMjZTRwtRPF2GmogeO/ArSsveGTJ85BEy9tRaMM7YTLkScDpkly51dIaMlR0h43i62vNLnxsxMq6uYJwkw12C5Atk/O5iolNkTJnzhXE6ZD9drT9HhoRKBBlX09VOktF7DiNk7KernSVD4cxhgIyvq0knytBMGTK26Wo7CtczTqP0WT2QYYPSsnemjFAhY1f2zpRxYe8g4+tZPafKIOfjMAw4mloLxuky2HuP2SFOC8bJMiZspr7K3tkyYqXjxPBSMrarSRYPbYf8UjJ0uppZGTX0ryRDC4ZdGaFvMl7mNrIxvU03qBNvI5syvdDIWMqe3ZHhmVle5WlkS8EwLIPodUbG4gIybFBaMKzLeJXSpwUDMmxQWjAgwwalBQMybFDNhYMMG9RSMCDDBLUsfgAZJqilYECGCcppwYAMG1TXXECGDUqnq0GGDWr+p71zUU4jB6Lo1aOjRq+ZrDf//6tLy4nKdskjbyBEQJ+KoShfWsqcMDPNgHL6BpWxRkqaPZWxRkoaDJWxRqotfqAylki1BkNlLJFqLlTGGilp9lTGGim5mqQy1khJg6Ey1kj9OL1AZayRkgZDZayR+n46GaiMJVLpn1OFylgiJQ1GUBlrpKTBUBlrpKTBuEcZ2XkuDyZDribdpYyUYXx+KBmy+MF9yhDc/kgy5ONq9ysje/NAMlqzd7cyDNcH+hpZPZ3qJLXu18hgXHmgsylpMIC7fWXE+ECntkYajPuVEYiZ66PIEBd3LOOhmj5p9lTGGim5mqQy1kjJx9VUxhopaTBUxhqp5kJlLJGSBuNatRJlANaTyx9ThoLKuOlCFDGKCyq7o/QxZZ3KmKSkwbieWNrlxrabj6lEWWUMUuMGY14rcvXkUnbEGaEweZeB3LY+O2AjAIFyi6JRaAOMjwDYqoxRatxgzGUQ140cl509QtxCdWSAQgFWFEQHeYR+CwT5RfSp+VEZw9S4wZjLyIClDaiUW8rIAzgOVAB4C8D+lGEgBMOu0g4AGxmVcbgQhflfMnzbzAnItIfg6IwFkIgcAFCX0bd8QCaKECollTFIjRuMuQz+tZkzyf6qhtC2PxzVJqP0HZSlXospQ9gpq4xxqn1cLeH3ZVi5R5NRib153U19PIAHFPKsr4xJqjUYF8iIcr+JjORjogiAI/Dx1DZTyVQAoJAewIep3mBcspuKe/FkAecNNgqAZbxt+gIF7Mx4PdWCcypjkOqLH1wiI2yeOHjbPLwaCZSbDU+cAVTKrydgECOgTWWMUn3xgyuPyAVvsO5taiejMgap3uxde8Tq8QZX36ac1femBqne7F1/RJs/SxlrVMYoJQ3GUvN6YhlnF2mteT2vDGkwFpvX08owL6fvq83raWWcm73l5vWsMs5Xk9ab15PKODcYC87rchm79/beZJwbjBXndbkMnw3f19fIvr+cG4wF53W5jOAAa4fP/vfl21e4eep0+mFW/EdyuYwagS0Ov7n0clqTl22p70p9Hvs9Gc/yfy6t/sp43U2pjCVk3OEBfNnUc57aLpp60qZvzZTKWCilMhZKqYyFUteVQfwOz2M09bUY4YqwpiapSUxlqAxN/WkZRVOD1GFMUZQbsTEFANb7HUJy3iUMiMyeIXhmBsZYz1z7G2EHKZcmtfrzo+c8XZA0EPNwuD7KvBYzUzmsFYn7FupTvCI5uyATMdlDsFb+DPn1C5/wKba8f4t4zD6v1Z9fHQJPFyQNDge0UWa1hEmtkLlvoT7Fq+JCG6Ddt3knfzDVqYzBxZMRNR7X6s+P2/GIcPsXZcxr7YzjWpn7FupTvLoMmWWsAEDtZ0jg/qIvBzugmN5fVhzjtuNa/fkuyM9sQdJA7DKG9FHmtWLBca3MfQv1KV4Dx0Kdy+jBWNBISBwwLpcAGz+faa9l3bjWXMZ4QVJjUD2G9FHmtSgNa81l3H435dPg0DCa8Pw1vLEZ1JrspmYLko5TfZRpLVQ3qHX73dT4AD6eqjEwvGNMAkqcH8B9mtaaH8CFGPu4wWNEH2VWS1zNav3hA3j15N2vU9sY0E/cxjvU5JD54CwyMrskKdSD8z7vmeOkVnt+KUD0HI4XJC0FhfnzlIwyrwVDBjisFT35LTn/9q+oKIqiKIqiKGNS66kyhEoZP8kRyu2xEdGiEVlP4/8uhgsbCMZnD1SH5FNwCMxsoNyUnXY0tggOQCxuQ3BwAeri1kRf+oWKYoHkHUSG5ZKg3JTM2ScASOTPANlzk4FsfYZyS3hHiQD61QwOsYiMDLgK5YY0ERzEw94e2gjjc3CIzFEPGoqiKIqiKIqiKMoC/AcFvuZMYBRo9AAAAABJRU5ErkJggg==)\n",
    "\n",
    "#### ***Hyperbolic Tangent***: This activation too is better than sigmoid fucntion when you are dealing with big data. Derivatives of 'tanh' is larger than sigmoid's, thus minimizing the cost. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "s-kPZoSWgfCW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import datasets\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nXWvSeLYgr6D"
   },
   "outputs": [],
   "source": [
    "d = datasets.load_digits()\n",
    "X = d.data\n",
    "y = d.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GtyNVBj_gvRX"
   },
   "outputs": [],
   "source": [
    "def ANN(optimizer = 'sgd',neurons = 32, batch_size = 64, epochs = 20, activation = 'relu', patience = 2, loss = 'categorical_crossentropy'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_shape=(X.shape[1],), activation=activation))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "\n",
    "    model.add(Dense(10,activation='softmax'))  \n",
    "    model.compile(optimizer = optimizer, loss=loss)\n",
    "    early_stopping = EarlyStopping(monitor=\"loss\", patience = patience)\n",
    "\n",
    "    history = model.fit(X, pd.get_dummies(y).values, batch_size=batch_size, epochs=epochs, callbacks = [early_stopping], verbose=0) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q6i-B2wAhhn5",
    "outputId": "9d124470-01b9-4d20-b741-37ac2ca59725"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1171\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0873\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0973\n"
     ]
    }
   ],
   "source": [
    "clf = KerasClassifier(build_fn=ANN, verbose=1)\n",
    "scores = cross_val_score(clf, X, y, cv=3,scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkrFpLMmhUhf",
    "outputId": "d9974f23-8cdb-4aad-f45d-8420c052aa9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is:0.9554813578185865\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model is:\"+ str(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoFfxgv_h-S_"
   },
   "source": [
    "**Accuracy #1 of the model is around 95.548%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqJ_9DqShr9P"
   },
   "source": [
    "# **Let us try tuning the model**\n",
    "#### Restart the kernels and run necessary cells above, that is, not the last 3 cells and run the below cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KJ0LxiOKh10f"
   },
   "outputs": [],
   "source": [
    "#observe that we have changed the number of neurons and batch size value as well as number of epochs too\n",
    "def ANN(optimizer = 'sgd',neurons = 128, batch_size = 32, epochs = 30, activation='relu', patience = 3, loss = 'categorical_crossentropy'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_shape=(X.shape[1],), activation=activation))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "\n",
    "    model.add(Dense(10,activation='softmax'))  \n",
    "    model.compile(optimizer = optimizer, loss=loss)\n",
    "    early_stopping = EarlyStopping(monitor=\"loss\", patience = patience)\n",
    "\n",
    "    history = model.fit(X, pd.get_dummies(y).values, batch_size=batch_size, epochs=epochs, callbacks = [early_stopping], verbose=0) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWRiMasOi2SW",
    "outputId": "230af436-d005-49da-8109-3971a65ddb07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0113\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0065\n"
     ]
    }
   ],
   "source": [
    "clf = KerasClassifier(build_fn=ANN, verbose=1)\n",
    "scores = cross_val_score(clf, X, y, cv=3,scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovIag6j4i7uv",
    "outputId": "377a3762-e393-447a-ec38-2e42f91746b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is:0.9994435169727324\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model is:\"+ str(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tKhN661i-q2"
   },
   "source": [
    "**Accuracy #2 of the model is nearly 100%!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9ltm2b6jKgV"
   },
   "source": [
    "### Conclusion:\n",
    "#### You have observed how changing certain hyperparameters have tweaked or improved the model's performace, bringing it close to 100%! Thus, from this notebook, you have learnt how important it is to implement hyperparameter tuning when you work on more complex dataset so as to improve your model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Hyperparameter_Tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
