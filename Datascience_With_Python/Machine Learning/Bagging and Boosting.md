# Bagging and Boosting

## Introduction
Bagging and Boosting are Ensemble techniques. In bagging and boosting, we utilize multiple models and predict the dataset by considering outputs of all models. Ensemble techniques mainly focused on training multiple models (called weak learners) to solve same problem and their prediction will be combined together to predict accurate result. The primary importance of this technique is that, when multiple models are correctly combined, we can obtain more robust model.

## Bagging
Bagging (also called Bootstrap Aggregation) is commonly used to reduce variance within a dataset. Let's understand this with an example:<br />Suppose we have a dataset named 'D'. We have 4 machine learning models named 'M1', 'M2', 'M3' and 'M4'. Now we will give small samples of data to all the 4 models. The strategy we will use to give data samples to models is called "Row sampling with replacement'. It means we will give small random samples to different models but data can be repeated in other model also. Suppose we gave 'D1', 'D2', 'D3', 'D4' to 'M1', 'M2', 'M3', 'M4' respectively. Now we will train all 4 models with all 4 sample data. After training is done, we will give our test data to each of these 4 models and check their prediction. Suppose '1', '0', '1', '1' is predicted by 'M1', 'M2', 'M3', 'M4' respectively. Bagging works on voting classification. It means our prediction will be majority of prediction among all predictions. In this example 1 will be our output since 3 models predicted '1' and 1 model predicted '0'.                       
