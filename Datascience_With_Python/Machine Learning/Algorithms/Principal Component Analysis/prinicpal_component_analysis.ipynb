{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prinicpal_component_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlYDlfUjoTLX"
      },
      "source": [
        "# **Principal Component Analysis** \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Principal Component Analysis (or PCA in short) is defined as a linear dimensionality reduction technique which is employed for extracting information from a high-dimensional space by projecting it into a lower-dimensional sub-space. It tries to preserve the essential parts that have more variation of the data and eliminate the non-essential parts with fewer variations.\n",
        "Now if you don't know what is dimension or variance? Then don't worry, at the end of this article you'll understand all the things.\n",
        "\n",
        "\n",
        "Dimensions are nothing but features that represent the parts of your data.\n",
        "*For instance,* a 30*30 image has 800 pixels that are the size or features which together represent that image.\n",
        "PCA is one of the most important topics of statistical procedures which is used for finding the inter-relation between variables in the given data & also to interpret and visualize the data.\n",
        "So before jumping on the detailed analysis of PCA, first let us understand some important terminologies of PCA for better understanding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5bEgzgvy9qj"
      },
      "source": [
        "# **Important Terminologies**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Principal Components**\n",
        "\n",
        "---\n",
        "\n",
        "Principal components are the key to PCA; they represent the fundamental essence of your data. During a layman term, when the info is projected into a lower dimension (assume three dimensions) from a far better space, the three dimensions are nothing but the three Principal Components that captures or holds most of the variance of your data.\n",
        "Principal components are comprises of both direction as well as magnitude. The direction represents across which principal axes the info is typically opened or has most variance and thus the magnitude signifies the number of variance that Principal Component captures of the info when projected onto that axis. The principal components form a straight line, and therefore the first principal component holds the foremost variance within the data. Each subsequent principal component is orthogonal to the last and features a lesser variance.\n",
        "\n",
        "*Remember, each principal component represents a percentage of total variation captured from the data.*\n",
        "\n",
        "So this is how for a given group of x correlated variables over y samples you achieve a group of u uncorrelated principal components over an equivalent y samples.\n",
        "The reason you achieve uncorrelated principal components from the first features is that the correlated features contribute to an equivalent principal component, thereby reducing the first data features into uncorrelated principal components; each representing a special set of correlated features with different amounts of variation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYvOSSNBzl-i"
      },
      "source": [
        "**Variance**\n",
        "\n",
        "---\n",
        "\n",
        "In machine learning, Variance is one among the foremost important factors that directly affect the accuracy of the output. When a machine learning model becomes too sensitive for the independent variables, it tries to seek out out the connection between every feature which provides rise to the matter like ‘overfitting’ or high variance. an excessive amount of noise enters the dataset due to high variance and thus results are affected. Once we use principal component analysis for dimensionality reduction, the matter of overfitting get’s solved simultaneously.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62AIiAovz98G"
      },
      "source": [
        "## **IMPORTANCE OF PRINCIPAL COMPONENT ANALYSIS**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Technically, Principal Component Analysis is essentially a statistical method to convert a group of observation of possibly correlated variables into a group of values of linearly uncorrelated variables.\n",
        "PCA is particularly used for dimensionality reduction during a dataset consisting of the various variables that are highly correlated or lightly correlated with each other while retaining the variation present within the dataset up to a maximum extent. it's also an excellent tool for exploratory data analysis for creating predictive models.\n",
        "\n",
        "In statistics, we all have heard that the more data we've , the more accurate results we are getting to observe, while it's rightly said but it’s not just data that we require, we'd like high-quality data to urge better results. Dimensionality reduction is essentially a way where we reduce the amount of columns or features from the dataset supported their relevance to the matter , the smallest amount their requirement, the more are the probabilities that they get faraway from the dataset.\n",
        "PCA performs a linear transformation on the data in order that most of the variance or information in your high-dimensional dataset is captured by the primary few principal components. the primary principal component will capture the foremost variance, followed by the second principal component, and so on.\n",
        "Each principal component may be a linear combination of the first variables. Because all the principal components are orthogonal to each other, there is no redundant or repeated information. So, the entire variance within the data is defined because the sum of the variances of the individual component. So decide the entire number of principal components consistent with cumulative variance ‘‘explained’’ by them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbG49TvV07Lm"
      },
      "source": [
        "# **USES OF PCA**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "There are many uses of PCA, but Data Visualization, Speeding ML Algorithms, \n",
        "Engine Health Monitoring and Wine Detection are the most important ones. So let’s understand them one by one:\n",
        " \n",
        "\n",
        "**1. Data Visualization:** When performing on any data related problem, the challenge  in today's world is that the sheer volume of knowledge, and therefore the variables/features that outline that data to unravel a drag where data is that the key, you would like extensive data exploration like checking out how the variables are correlated or understanding the distribution of a couple of variables. Considering that there are an outsized number of variables or dimensions along which the data is distributed, visualization are often a challenge and almost impossible.\n",
        "Hence, PCA can do that for you since it projects the info into a lower dimension, thereby allowing you to ascertain the info during a 2D or 3D space.\n",
        "\n",
        "**2.Speeding Machine Learning (ML) Algorithm:** Since PCA's main idea is \n",
        "dimensionality reduction, you'll leverage that to hurry up your machine learning algorithm's training and testing time considering your data features a lot of features, and therefore the ML algorithm's learning is just too slow.\n",
        "\n",
        "**3.Engine Health Monitoring:**\n",
        "You have a dataset that has measurements for various sensors on an engine (temperatures, pressures, emissions, then on). While much of the info comes from a healthy engine, the sensors have also captured data from the engine when it needs maintenance. you can't see any obvious abnormalities by watching a person sensor. However, by applying PCA, you'll transform this data in order that most variations within the sensor measurements are captured by alittle number of principal components. it's easier to differentiate between a healthy and unhealthy engine by inspecting these principal components than by watching the raw sensor data.\n",
        "\n",
        "**4.Wine Detection:**\n",
        "You have a dataset that has measurements for various variables on wine (alcohol, ash, magnesium, then on). You can't see any obvious abnormalities by watching a person variables. However, by applying PCA, you'll transform this data in order that most variations within the measurements of the variables are captured by a little number of principal components. it's easier to differentiate between red and wine by inspecting these principal components than by watching the raw variable data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wej-IxMu45C"
      },
      "source": [
        "## **Steps in PCA Algorithm**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1.   **Getting the dataset-** Originally, we need to take the input dataset and divide it into two subparts X and Y, where X is the training set, and Y is the test set. \n",
        "\n",
        "2.  **Representing data into a structure-**\n",
        " Now we will represent our dataset into a structure. Similar as we will represent the two-dimensional matrix of independent variable X. Then each row corresponds to the data particulars, and the column corresponds to the Features. The number of columns is the confines of the dataset. \n",
        "3. **Normalization-** In this step, we will normalize our dataset. Similar as in a particular column, the features with high friction are more important compared to the features with lower friction. \n",
        " Still, also we will divide each data item in a column with the standard divagation of the column, If the significance of features is independent of the friction of the point. Then we will name the matrix as Z. \n",
        "4. **Calculating the Covariance of Z-** \n",
        "To calculate the covariance of Z, we will take the matrix Z, and will transpose it. After transpose, we will multiply it by Z. The affair matrix will be the Covariance matrix ofZ. \n",
        "5. **Calculating the Eigen Values and Eigen Vectors-** \n",
        " Now we need to calculate the eigenvalues and eigenvectors for the attendant covariance matrixZ. Eigenvectors or the covariance matrix are the directions of the axes with high information. And the portions of these eigenvectors are defined as the eigenvalues. \n",
        "6. **Sorting the Eigen Vectors-**In this step, we will take all the eigenvalues and will sort them in dwindling order, which means from largest to lowest. And contemporaneously sort the eigenvectors consequently in matrix P of eigenvalues. The attendant matrix will be named as P*. \n",
        "7. **Calculating the Principal COmponents-**\n",
        " Then we will calculate the new features. To do this, we will multiply the P * matrix to the Z. In the attendant matrix Z *, each observation is the direct combination of original features. Each column of the Z * matrix is independent of each other. \n",
        "8. **Remove insignificant features from the new dataset-**\n",
        "Now we will decide then what to keep and what to remove. It means, we will only keep the applicable or important features in the new dataset, and insignificant features will be removed out. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCHuvvsqV-Q7"
      },
      "source": [
        "## **Implementation of PCA**\n",
        "\n",
        "PCA can be caluculated easily on any dataset by using the PCA-class of scikit learn library. The key benefit of this approach is that once the projection is calculated, it can be applied to new data again and again quite easily.\n",
        "\n",
        "When creating the class, the number of components can be specified as a parameter.\n",
        "However, PCA can also be calculate manually by using some of the functions of numPy library. So first, let us see how to calculate PCA manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1KQv1iXIhck"
      },
      "source": [
        "## **Manual Implementation of PCA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzKcaR-gqDt5",
        "outputId": "c75aab95-a718-4b9d-d6e0-3720c490b0c2"
      },
      "source": [
        "from numpy import array\n",
        "from numpy import mean\n",
        "from numpy import cov\n",
        "from numpy.linalg import eig\n",
        "# define a matrix\n",
        "A = array([[1, 2], [3, 4], [5, 6]])\n",
        "print(A)\n",
        "# calculate the mean of each column\n",
        "M = mean(A.T, axis=1)\n",
        "print(M)\n",
        "# center columns by subtracting column means\n",
        "C = A - M\n",
        "print(C)\n",
        "# calculate covariance matrix of centered matrix\n",
        "V = cov(C.T)\n",
        "print(V)\n",
        "# eigendecomposition of covariance matrix\n",
        "values, vectors = eig(V)\n",
        "print(vectors)\n",
        "print(values)\n",
        "# project data\n",
        "P = vectors.T.dot(C.T)\n",
        "print(P.T)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2]\n",
            " [3 4]\n",
            " [5 6]]\n",
            "[3. 4.]\n",
            "[[-2. -2.]\n",
            " [ 0.  0.]\n",
            " [ 2.  2.]]\n",
            "[[4. 4.]\n",
            " [4. 4.]]\n",
            "[[ 0.70710678 -0.70710678]\n",
            " [ 0.70710678  0.70710678]]\n",
            "[8. 0.]\n",
            "[[-2.82842712  0.        ]\n",
            " [ 0.          0.        ]\n",
            " [ 2.82842712  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jhCFEOlslLs"
      },
      "source": [
        "The example above defines a small 3×2 matrix, centers the data in the matrix, calculates the covariance matrix of the centered data, and then the eigendecomposition of the covariance matrix. The eigenvectors and eigenvalues are taken as the principal components and singular values and used to project the original data.\n",
        "\n",
        "In the output, we can see that only the first eigenvector is required, suggesting that we could project our 3×2 matrix onto a 3×1 matrix with little loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNzX6n9as7ws"
      },
      "source": [
        "Now let's calculate the PCA on a dataset using the PCA-class of scikit-learn library. So first we need to import PCA from the respective library and then the class is first fit on a dataset by calling the fit() function, and then the original dataset or other data can be projected into a subspace with the chosen number of dimensions by calling the transform() function.\n",
        "\n",
        "Once fit, the eigenvalues and principal components can be accessed on the PCA class via the explained_variance_ and components_ attributes.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6f3Cj0rIwQP"
      },
      "source": [
        "## **Implementation with PCA class(Reusable Implementation)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfqMtTnZqSQa",
        "outputId": "2e7a9e21-5e6b-487b-b630-cafb0cefaabc"
      },
      "source": [
        " #Principal Component Analysis\n",
        "from numpy import array\n",
        "from sklearn.decomposition import PCA\n",
        "# define a matrix\n",
        "A = array([[1, 2], [3, 4], [5, 6]])\n",
        "print(A)\n",
        "# create the PCA instance\n",
        "pca = PCA(2)\n",
        "# fit on data\n",
        "pca.fit(A)\n",
        "# access values and vectors\n",
        "print(pca.components_)\n",
        "print(pca.explained_variance_)\n",
        "# transform data\n",
        "B = pca.transform(A)\n",
        "print(B)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2]\n",
            " [3 4]\n",
            " [5 6]]\n",
            "[[ 0.70710678  0.70710678]\n",
            " [-0.70710678  0.70710678]]\n",
            "[8. 0.]\n",
            "[[-2.82842712e+00 -2.22044605e-16]\n",
            " [ 0.00000000e+00  0.00000000e+00]\n",
            " [ 2.82842712e+00  2.22044605e-16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hV-EpEt-NkC"
      },
      "source": [
        "\n",
        "The example above demonstrates using this class by first creating an instance, fitting it on a 3×2 matrix, accessing the values and vectors of the projection, and transforming the original data. \n",
        "\n",
        "We can see, that with some very minor floating point rounding that we achieve the same principal components, singular values, and projection as in the previous example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiR-O3IiA0OD"
      },
      "source": [
        "However, if you want to implement PCA with a dataset, you can do that easily by importing the dataset in your code. First import these two libraries:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvesFvV2HdFo"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGh9Wjz1HiTp"
      },
      "source": [
        "Then import your dataset. If you are using a local dateset, then make sure your dataset should be placed in the same directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqaHytpEH869"
      },
      "source": [
        "df=pd.read_csv('../input/dataset.csv') #Replace it with your path where the data file is stored"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIwsa7-MIDpZ"
      },
      "source": [
        "After doing this follow the same steps as in the previous example for fitting and trasnformming the data and it's done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltyjr169HbOS"
      },
      "source": [
        "So that's all for my side. I hope this article help you to understand principal component analysis. Thank you!"
      ]
    }
  ]
}