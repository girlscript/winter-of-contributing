{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science with Python : ID3 Classification Algorithm #1822"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic simple algorithm used in building decision trees is known as the ID3 classification algorithm. It builds decision trees using a top-down, greedy approach. Decision tree has decision nodes and leaf nodes. A decision node has two or more brunches and leaf node represents a decision. The leaf nodes can not be divided more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3 Algorithm :\n",
    "This algorithm assumes that all the attributes are discrete and the classification is binary.\n",
    "###### Step 1. \n",
    "Calculate the entropy or amount of uncertainity in the data set.\n",
    "\n",
    "$$Entropy(S) = \\frac{-p}{p+n}log_{2}(\\frac{p}{p+n}) - \\frac{n}{p+n}log_{2}(\\frac{n}{p+n})$$\n",
    "###### Step 2. \n",
    "Then find the Average Information Entropy:\n",
    "\n",
    "$$I(Attribute) = \\sum\\frac{p_{i}+n_{i}}{p+n}Entropy(A)$$\n",
    "###### Step 3. \n",
    "Find the information gain for each attribute. It is the difference between entropy before splitting and average entropy after splitting.\n",
    "\n",
    "$$Gain = Entropy(S) - I(Attribute)$$\n",
    "\n",
    "###### Step 4. \n",
    "Select the Highest Information Gain Attribute.\n",
    "\n",
    "###### Step 5. \n",
    "Now just repeat until the desired tree is formed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Decision Tree :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see the step by step implementation of ID3 algorithm. we will use a simple dataset to build a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Our Data Set is:\n",
      "      Outlook Temperature Humidity Play Tennis\n",
      "0      Sunny         Hot     High          No\n",
      "1   Overcast         Hot     High         Yes\n",
      "2       Rain        Mild     High         Yes\n",
      "3       Rain        Cool   Normal         Yes\n",
      "4   Overcast        Cool   Normal         Yes\n",
      "5      Sunny        Mild     High          No\n",
      "6      Sunny        Cool   Normal         Yes\n",
      "7       Rain        Mild   Normal         Yes\n",
      "8      Sunny        Mild   Normal         Yes\n",
      "9   Overcast        Mild     High         Yes\n",
      "10  Overcast         Hot   Normal         Yes\n"
     ]
    }
   ],
   "source": [
    "#We will use pandas for manipulating the dataset.\n",
    "import pandas as pd\n",
    "\n",
    "#We will read the dataset (csv file) and load it into pandas dataframe.\n",
    "my_dataframe = pd.read_csv('Downloads/PlayTennis.csv')\n",
    "print(\"\\n Our Data Set is:\\n\", my_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Target Attribute is --->  Play Tennis\n",
      "Our Predicting Attributes --->  ['Outlook', 'Temperature', 'Humidity']\n"
     ]
    }
   ],
   "source": [
    "# Now we will fetch the attribute names from input dataset\n",
    "target = my_dataframe.keys()[-1]\n",
    "print('Our Target Attribute is ---> ', target)\n",
    "attribute_names = list(my_dataframe.keys())\n",
    "\n",
    "#Now we will remove our target attribute from the attribute names list\n",
    "attribute_names.remove(target) \n",
    "print('Our Predicting Attributes ---> ', attribute_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build a decision tree we must compare the impurity of each attribute\n",
    "# So we have to find out what is the entropy of our collection\n",
    "# For this we can define a function to calculate the entropy using -x*log2*x\n",
    "\n",
    "import math  \n",
    "def entropy(probabilities):  \n",
    "    return sum( [-probability*math.log(probability, 2) for probability in probabilities])\n",
    "\n",
    "# We have to calculate the entropy of the dataset w.r.t target attribute\n",
    "# We will do the calculation with the help of a function\n",
    "\n",
    "def entropy_of_list(lst,value):  \n",
    "    from collections import Counter\n",
    "    \n",
    "    #Initialize Total instances associated with respective attribute\n",
    "    total_instances = len(lst)  \n",
    "    print(\".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\")\n",
    "    print(\"\\nTotal no of instances associated with '{0}' is ---> {1}\".format(value,total_instances))\n",
    "    # Now we will calculate the proportion of class using count variable\n",
    "    count = Counter(x for x in lst)\n",
    "    print('\\nTarget attribute class count(Yes/No)=',dict(count))\n",
    "    \n",
    "    # Here, x means number of YES/NO\n",
    "    probabilities = [x / total_instances for x in count.values()]  \n",
    "    print(\"\\nClasses --->\", max(count), min(count))\n",
    "    print(\"\\nProbabilities of Class 'p'='{0}' ---> {1}\".format(max(count),max(probabilities)))\n",
    "    print(\"Probabilities of Class 'n'='{0}' ---> {1}\".format(min(count),min(probabilities)))\n",
    "    \n",
    "    return entropy(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will calculate Information Gain using a funtion\n",
    "# The function will find out difference in Entropy before and after splitting the dataset\n",
    "\n",
    "def information_gain(my_dataframe, split_attribute, target, battr):\n",
    "    print(\"\\n\\n.~.~.~.~. Information Gain Calculation of\",split_attribute,\".~.~.~.~. \") \n",
    "    \n",
    "    # group the data based on attribute values\n",
    "    my_dataframe_split = my_dataframe.groupby(split_attribute) \n",
    "    glist=[]\n",
    "    for gname,group in my_dataframe_split:\n",
    "        print('Our Grouped Attribute Values \\n',group)\n",
    "        print(\".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\")\n",
    "        glist.append(gname) \n",
    "    \n",
    "    #Let's calculate the entropy and proportion \n",
    "    glist.reverse()\n",
    "    nobs = len(my_dataframe.index) * 1.0   \n",
    "    my_dataframe_agg1=my_dataframe_split.agg({target :lambda x:entropy_of_list(x, glist.pop())})\n",
    "    my_dataframe_agg2=my_dataframe_split.agg({target :lambda x:len(x)/nobs})\n",
    "    \n",
    "    my_dataframe_agg1.columns=['Entropy']\n",
    "    my_dataframe_agg2.columns=['Proportion']\n",
    "    \n",
    "    # Calculate Information Gain:\n",
    "    # We just have to find difference in new and old entropy\n",
    "    new_entropy = sum( my_dataframe_agg1['Entropy'] * my_dataframe_agg2['Proportion'])\n",
    "    if battr !='S':\n",
    "        old_entropy = entropy_of_list(my_dataframe[target],'S-'+my_dataframe.iloc[0][my_dataframe.columns.get_loc(battr)])\n",
    "    else:\n",
    "        old_entropy = entropy_of_list(my_dataframe[target],battr)\n",
    "    return old_entropy - new_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here goes our ID3 Algorithm\n",
    "# Using this function first we will check if the split we are going to do is best or not\n",
    "# Then we must compare the information gain to pick up the best and after that will start splitting\n",
    "# We will Initiate the tree choosing the best attribute to be a node\n",
    "# Will split dataset-On each split and recursively call this algorithm.\n",
    "# We have to complete the empty tree with sub-trees, by  calling the functions recursively\n",
    "\n",
    "def id3(my_dataframe, target, attribute_names, default_class=None,default_attr='S'):\n",
    "    \n",
    "    from collections import Counter\n",
    "    count = Counter(x for x in my_dataframe[target])# x is the number of YES/NO\n",
    "    \n",
    "    # First we have to check that this split of the dataset is homogeneous or not\n",
    "    if len(count) == 1:\n",
    "        return next(iter(count))  # next input data set, or raises StopIteration when EOF is hit.\n",
    "    \n",
    "    # Then we must chceck if this split of the dataset is empty or not. \n",
    "    # If yes, we will return a default value\n",
    "    elif my_dataframe.empty or (not attribute_names):\n",
    "        return default_class  # We are Returning None for Empty Data Set\n",
    "    \n",
    "    # Finally This dataset is ready\n",
    "    else:\n",
    "        # Set a Default Value for next recursive call of this function:\n",
    "        default_class = max(count.keys()) #No of YES and NO Class\n",
    "        # Computing the Information Gain of the attributes:\n",
    "        gains=[]\n",
    "        for attr in attribute_names:\n",
    "            ig= information_gain(my_dataframe, attr, target, default_attr)\n",
    "            gains.append(ig)\n",
    "            print('\\nInformation gain of','“',attr,'”','is ---> ', ig)\n",
    "            print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "            print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "         \n",
    "        index_of_max = gains.index(max(gains))               # Index of Best Attribute\n",
    "        best_attr = attribute_names[index_of_max]            # Choose Best Attribute to split on\n",
    "        print(\"\\nList of Gain for arrtibutes:\", attribute_names,\"\\nare:\", gains,\"respectively.\")\n",
    "        print(\"\\nAttribute with the maximum gain is ---> \", best_attr)\n",
    "        print(\"\\nHence, the Root node will be ---> \", best_attr)\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "\n",
    "        # We will Create an empty tree first\n",
    "        # Gradually it will become populated \n",
    "        tree = {best_attr:{}} # Initiating the tree with the best attribute as a node \n",
    "        remaining_attribute_names =[i for i in attribute_names if i != best_attr]\n",
    "        \n",
    "        # Complete the tree\n",
    "        for attr_val, data_subset in my_dataframe.groupby(best_attr):\n",
    "            subtree = id3(data_subset,target, remaining_attribute_names,default_class,best_attr)\n",
    "            tree[best_attr][attr_val] = subtree\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      ".~.~.~.~. Information Gain Calculation of Outlook .~.~.~.~. \n",
      "Our Grouped Attribute Values \n",
      "      Outlook Temperature Humidity Play Tennis\n",
      "1   Overcast         Hot     High         Yes\n",
      "4   Overcast        Cool   Normal         Yes\n",
      "9   Overcast        Mild     High         Yes\n",
      "10  Overcast         Hot   Normal         Yes\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "Our Grouped Attribute Values \n",
      "   Outlook Temperature Humidity Play Tennis\n",
      "2    Rain        Mild     High         Yes\n",
      "3    Rain        Cool   Normal         Yes\n",
      "7    Rain        Mild   Normal         Yes\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "Our Grouped Attribute Values \n",
      "   Outlook Temperature Humidity Play Tennis\n",
      "0   Sunny         Hot     High          No\n",
      "5   Sunny        Mild     High          No\n",
      "6   Sunny        Cool   Normal         Yes\n",
      "8   Sunny        Mild   Normal         Yes\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "\n",
      "Total no of instances associated with 'Overcast' is ---> 4\n",
      "\n",
      "Target attribute class count(Yes/No)= {'Yes': 4}\n",
      "\n",
      "Classes ---> Yes Yes\n",
      "\n",
      "Probabilities of Class 'p'='Yes' ---> 1.0\n",
      "Probabilities of Class 'n'='Yes' ---> 1.0\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "\n",
      "Total no of instances associated with 'Rain' is ---> 3\n",
      "\n",
      "Target attribute class count(Yes/No)= {'Yes': 3}\n",
      "\n",
      "Classes ---> Yes Yes\n",
      "\n",
      "Probabilities of Class 'p'='Yes' ---> 1.0\n",
      "Probabilities of Class 'n'='Yes' ---> 1.0\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "\n",
      "Total no of instances associated with 'Sunny' is ---> 4\n",
      "\n",
      "Target attribute class count(Yes/No)= {'No': 2, 'Yes': 2}\n",
      "\n",
      "Classes ---> Yes No\n",
      "\n",
      "Probabilities of Class 'p'='Yes' ---> 0.5\n",
      "Probabilities of Class 'n'='No' ---> 0.5\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "\n",
      "Total no of instances associated with 'S' is ---> 11\n",
      "\n",
      "Target attribute class count(Yes/No)= {'No': 2, 'Yes': 9}\n",
      "\n",
      "Classes ---> Yes No\n",
      "\n",
      "Probabilities of Class 'p'='Yes' ---> 0.8181818181818182\n",
      "Probabilities of Class 'n'='No' ---> 0.18181818181818182\n",
      "\n",
      "Information gain of “ Outlook ” is --->  0.320402072002678\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "\n",
      ".~.~.~.~. Information Gain Calculation of Temperature .~.~.~.~. \n",
      "Our Grouped Attribute Values \n",
      "     Outlook Temperature Humidity Play Tennis\n",
      "3      Rain        Cool   Normal         Yes\n",
      "4  Overcast        Cool   Normal         Yes\n",
      "6     Sunny        Cool   Normal         Yes\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "Our Grouped Attribute Values \n",
      "      Outlook Temperature Humidity Play Tennis\n",
      "0      Sunny         Hot     High          No\n",
      "1   Overcast         Hot     High         Yes\n",
      "10  Overcast         Hot   Normal         Yes\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "Our Grouped Attribute Values \n",
      "     Outlook Temperature Humidity Play Tennis\n",
      "2      Rain        Mild     High         Yes\n",
      "5     Sunny        Mild     High          No\n",
      "7      Rain        Mild   Normal         Yes\n",
      "8     Sunny        Mild   Normal         Yes\n",
      "9  Overcast        Mild     High         Yes\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "\n",
      "Total no of instances associated with 'Cool' is ---> 3\n",
      "\n",
      "Target attribute class count(Yes/No)= {'Yes': 3}\n",
      "\n",
      "Classes ---> Yes Yes\n",
      "\n",
      "Probabilities of Class 'p'='Yes' ---> 1.0\n",
      "Probabilities of Class 'n'='Yes' ---> 1.0\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "\n",
      "Total no of instances associated with 'Hot' is ---> 3\n",
      "\n",
      "Target attribute class count(Yes/No)= {'No': 1, 'Yes': 2}\n",
      "\n",
      "Classes ---> Yes No\n",
      "\n",
      "Probabilities of Class 'p'='Yes' ---> 0.6666666666666666\n",
      "Probabilities of Class 'n'='No' ---> 0.3333333333333333\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "\n",
      "Total no of instances associated with 'Mild' is ---> 5\n",
      "\n",
      "Target attribute class count(Yes/No)= {'Yes': 4, 'No': 1}\n",
      "\n",
      "Classes ---> Yes No\n",
      "\n",
      "Probabilities of Class 'p'='Yes' ---> 0.8\n",
      "Probabilities of Class 'n'='No' ---> 0.2\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "\n",
      "Total no of instances associated with 'S' is ---> 11\n",
      "\n",
      "Target attribute class count(Yes/No)= {'No': 2, 'Yes': 9}\n",
      "\n",
      "Classes ---> Yes No\n",
      "\n",
      "Probabilities of Class 'p'='Yes' ---> 0.8181818181818182\n",
      "Probabilities of Class 'n'='No' ---> 0.18181818181818182\n",
      "\n",
      "Information gain of “ Temperature ” is --->  0.1054449832208344\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "\n",
      ".~.~.~.~. Information Gain Calculation of Humidity .~.~.~.~. \n",
      "Our Grouped Attribute Values \n",
      "     Outlook Temperature Humidity Play Tennis\n",
      "0     Sunny         Hot     High          No\n",
      "1  Overcast         Hot     High         Yes\n",
      "2      Rain        Mild     High         Yes\n",
      "5     Sunny        Mild     High          No\n",
      "9  Overcast        Mild     High         Yes\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "Our Grouped Attribute Values \n",
      "      Outlook Temperature Humidity Play Tennis\n",
      "3       Rain        Cool   Normal         Yes\n",
      "4   Overcast        Cool   Normal         Yes\n",
      "6      Sunny        Cool   Normal         Yes\n",
      "7       Rain        Mild   Normal         Yes\n",
      "8      Sunny        Mild   Normal         Yes\n",
      "10  Overcast         Hot   Normal         Yes\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "\n",
      "Total no of instances associated with 'High' is ---> 5\n",
      "\n",
      "Target attribute class count(Yes/No)= {'No': 2, 'Yes': 3}\n",
      "\n",
      "Classes ---> Yes No\n",
      "\n",
      "Probabilities of Class 'p'='Yes' ---> 0.6\n",
      "Probabilities of Class 'n'='No' ---> 0.4\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "\n",
      "Total no of instances associated with 'Normal' is ---> 6\n",
      "\n",
      "Target attribute class count(Yes/No)= {'Yes': 6}\n",
      "\n",
      "Classes ---> Yes Yes\n",
      "\n",
      "Probabilities of Class 'p'='Yes' ---> 1.0\n",
      "Probabilities of Class 'n'='Yes' ---> 1.0\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "\n",
      "Total no of instances associated with 'S' is ---> 11\n",
      "\n",
      "Target attribute class count(Yes/No)= {'No': 2, 'Yes': 9}\n",
      "\n",
      "Classes ---> Yes No\n",
      "\n",
      "Probabilities of Class 'p'='Yes' ---> 0.8181818181818182\n",
      "Probabilities of Class 'n'='No' ---> 0.18181818181818182\n",
      "\n",
      "Information gain of “ Humidity ” is --->  0.24269725634146505\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "List of Gain for arrtibutes: ['Outlook', 'Temperature', 'Humidity'] \n",
      "are: [0.320402072002678, 0.1054449832208344, 0.24269725634146505] respectively.\n",
      "\n",
      "Attribute with the maximum gain is --->  Outlook\n",
      "\n",
      "Hence, the Root node will be --->  Outlook\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "\n",
      ".~.~.~.~. Information Gain Calculation of Temperature .~.~.~.~. \n",
      "Our Grouped Attribute Values \n",
      "   Outlook Temperature Humidity Play Tennis\n",
      "6   Sunny        Cool   Normal         Yes\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "Our Grouped Attribute Values \n",
      "   Outlook Temperature Humidity Play Tennis\n",
      "0   Sunny         Hot     High          No\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "Our Grouped Attribute Values \n",
      "   Outlook Temperature Humidity Play Tennis\n",
      "5   Sunny        Mild     High          No\n",
      "8   Sunny        Mild   Normal         Yes\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "\n",
      "Total no of instances associated with 'Cool' is ---> 1\n",
      "\n",
      "Target attribute class count(Yes/No)= {'Yes': 1}\n",
      "\n",
      "Classes ---> Yes Yes\n",
      "\n",
      "Probabilities of Class 'p'='Yes' ---> 1.0\n",
      "Probabilities of Class 'n'='Yes' ---> 1.0\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "\n",
      "Total no of instances associated with 'Hot' is ---> 1\n",
      "\n",
      "Target attribute class count(Yes/No)= {'No': 1}\n",
      "\n",
      "Classes ---> No No\n",
      "\n",
      "Probabilities of Class 'p'='No' ---> 1.0\n",
      "Probabilities of Class 'n'='No' ---> 1.0\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "\n",
      "Total no of instances associated with 'Mild' is ---> 2\n",
      "\n",
      "Target attribute class count(Yes/No)= {'No': 1, 'Yes': 1}\n",
      "\n",
      "Classes ---> Yes No\n",
      "\n",
      "Probabilities of Class 'p'='Yes' ---> 0.5\n",
      "Probabilities of Class 'n'='No' ---> 0.5\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "\n",
      "Total no of instances associated with 'S-Sunny' is ---> 4\n",
      "\n",
      "Target attribute class count(Yes/No)= {'No': 2, 'Yes': 2}\n",
      "\n",
      "Classes ---> Yes No\n",
      "\n",
      "Probabilities of Class 'p'='Yes' ---> 0.5\n",
      "Probabilities of Class 'n'='No' ---> 0.5\n",
      "\n",
      "Information gain of “ Temperature ” is --->  0.5\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "\n",
      ".~.~.~.~. Information Gain Calculation of Humidity .~.~.~.~. \n",
      "Our Grouped Attribute Values \n",
      "   Outlook Temperature Humidity Play Tennis\n",
      "0   Sunny         Hot     High          No\n",
      "5   Sunny        Mild     High          No\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "Our Grouped Attribute Values \n",
      "   Outlook Temperature Humidity Play Tennis\n",
      "6   Sunny        Cool   Normal         Yes\n",
      "8   Sunny        Mild   Normal         Yes\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "\n",
      "Total no of instances associated with 'High' is ---> 2\n",
      "\n",
      "Target attribute class count(Yes/No)= {'No': 2}\n",
      "\n",
      "Classes ---> No No\n",
      "\n",
      "Probabilities of Class 'p'='No' ---> 1.0\n",
      "Probabilities of Class 'n'='No' ---> 1.0\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "\n",
      "Total no of instances associated with 'Normal' is ---> 2\n",
      "\n",
      "Target attribute class count(Yes/No)= {'Yes': 2}\n",
      "\n",
      "Classes ---> Yes Yes\n",
      "\n",
      "Probabilities of Class 'p'='Yes' ---> 1.0\n",
      "Probabilities of Class 'n'='Yes' ---> 1.0\n",
      ".~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
      "\n",
      "Total no of instances associated with 'S-Sunny' is ---> 4\n",
      "\n",
      "Target attribute class count(Yes/No)= {'No': 2, 'Yes': 2}\n",
      "\n",
      "Classes ---> Yes No\n",
      "\n",
      "Probabilities of Class 'p'='Yes' ---> 0.5\n",
      "Probabilities of Class 'n'='No' ---> 0.5\n",
      "\n",
      "Information gain of “ Humidity ” is --->  1.0\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "List of Gain for arrtibutes: ['Temperature', 'Humidity'] \n",
      "are: [0.5, 1.0] respectively.\n",
      "\n",
      "Attribute with the maximum gain is --->  Humidity\n",
      "\n",
      "Hence, the Root node will be --->  Humidity\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "The Resultant Decision Tree is:\n",
      "{'Outlook': {'Overcast': 'Yes',\n",
      "             'Rain': 'Yes',\n",
      "             'Sunny': {'Humidity': {'High': 'No', 'Normal': 'Yes'}}}}\n"
     ]
    }
   ],
   "source": [
    "#Now we will form our tree\n",
    "#Let's print our tree calling id3() funcion\n",
    "\n",
    "from pprint import pprint\n",
    "tree = id3(my_dataframe,target,attribute_names)\n",
    "print(\"\\nThe Resultant Decision Tree is:\")\n",
    "pprint(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have learnt how Id3 algorithm works by selecting the attribute with highest information gain (or we can say least entropy) iteratively. It can build small trees but we can not ensure that it is the smallest tree. As ID3 follows greedy method ,there are also high chances of getting stuck into local optimums. In spite of all these shortcomings ,ID3 algorithm is very much useful in bulding decision tree of a given dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
