
<b>Topic: Apriori Algorithm for data mining.</b>

<b>Content</b>
1. What are itemsets and frequent itemsets
2. What does apriori algorithm do?
3. Applications and Advantages
4. Important terms
5. Steps
6. Example of how algorithm works
7. Limitations
8. Summary

<b>What are itemsets and frequent itemsets?</b><br>
When different kinds of items together form a set, it is called an itemset. If it contains k different types of items, then it is precisely called 'k-itemset'.When in a certain amount of transactions, a particular itemset is occuring frequently, it is called a frequent itemset.<br>
Now the question arises, how can we know if an itemset is occuring frequently? This is measured with the help of a quantity termed as "Minimum support threshold".
If number of occurance of an itemset is greater than or equal to this specified threshold, it is said to be occuring frequently.<br><br>

<b>What does Apriori algorithm do?</b><br>
Apriori algorithm discovers relation between different items in a dataset.It checks on which items together are forming strong association rules and hence can be called frequent itemsets.<br><br>

<b>Applications</b><br>
You can use this for a number of real world problems like below:-<br>
1.Market Basket analysis - Shop owners identify patterns in customer behaviour and find out which products are frequently being bought together in order to think about new schemes and come up with new strategies.<br>
2.Recommendation systems - Based on user history, items, music, movies etc. can be recommended to a new user.<br>
3.Health pattern recognition - Many health related issues can have a pattern and researchers are using this apriori algorithm to find out possible relationship between them<br>
4.In Forestry- Analysis of probability and intensity of forest fire with the forest fire data.<br>
These are some of the many applications of this algorithm.<br><br>
<b>Advantages</b><br>
->Easy to understand algorithm<br>
->Join and Prune steps are easy to implement on large itemsets in large databases<br>
->The resulting rules are intuitive and easy to communicate to an end user<br>
->It doesn't require labeled data as it is fully unsupervised; as a result, you can use it in many different situations because unlabeled data is often more accessible<br>
->The algorithm is exhaustive, so it finds all the rules with the specified support and confidence<br>

<b>Important terms</b><br>
There are a few terms that we have to be familiar with before proceeding to understand the algorithm.They are listed here:<br>
<b>1.Support</b> - Suppose the support percentage of item x and y is given as 2%, it means that in 2% of the total transactions x and y were bought together. <br>
<b>2.Confidence</b> - Suppose the confidence percentage of items x and y is given to be 60%, then it means that about 60% customers who bought x as well as y.<br>
Both these quantities can be calculated as follows:<br>
![image1](https://www.softwaretestinghelp.com/wp-content/qa/uploads/2019/09/Support-and-Confidence-for-Itemset-A-and-B.png)<br>
<b>3.Join</b> - The join step find all the occurances of a k-itemset in every iteration<br>
<b>4.Prune</b> - This step scans the count of each item in the database. If the candidate item does not meet minimum support, then it is regarded as infrequent and thus it is removed.<br>

<b>Steps</b><br>
<b>#1</b> In the first iteration of the algorithm, each item is taken as a 1-itemsets candidate. The algorithm will count the occurrences of each item.

<b>#2</b> Let there be some specified minimum support, say 2. The set of 1 – itemsets whose occurrence is satisfying the min sup are determined. Only those candidates which count more than or equal to 2, are taken ahead for the next iteration and the others are pruned.

<b>#3</b> Next, 2-itemset frequent items with min_sup are discovered. For this in the join step, the 2-itemset is generated by forming a group of 2 by combining items with itself.

<b>#4</b> The 2-itemset candidates are pruned using min-sup threshold value. Now the table will have 2 –itemsets with min-sup only.

<b>#5</b> This process goes on with increasing number of k in k-itemsets until the most frequent itemset is achieved.<br><br>
<b>EXAMPLE</b><br>
Let us understand this by the following example.<br>
Let their be a set of 6 transactions of items <b>l1,l2,l3,l4,l5</b>. We have specified min_sup=3 and minimun confidence level to be 60%.<br><br>
![image2](https://github.com/rachita11/winter-of-contributing/blob/Datascience_With_Python/Datascience_With_Python/Machine%20Learning/Algorithms/Apriori%20Algorithm/Images/table%201.png)
<br>
1. First we find the number of occurances of each item in these transactions and table it down.
![image3](https://github.com/rachita11/winter-of-contributing/blob/Datascience_With_Python/Datascience_With_Python/Machine%20Learning/Algorithms/Apriori%20Algorithm/Images/table%202.png)<br>
2. We can see that some of the list elements have a count less than min_sup of value 3. Hence, we will prune those items and make a new table.
 ![image4](https://github.com/rachita11/winter-of-contributing/blob/Datascience_With_Python/Datascience_With_Python/Machine%20Learning/Algorithms/Apriori%20Algorithm/Images/table%203.png)<br>
3. Now using these many items, we will join 2-itemsets and find out their counts.
![image5](https://github.com/rachita11/winter-of-contributing/blob/Datascience_With_Python/Datascience_With_Python/Machine%20Learning/Algorithms/Apriori%20Algorithm/Images/table%204.png)<br>
4. Again in the next step, the rows having less than required count will get pruned, giving us the table below.
![image6](https://github.com/rachita11/winter-of-contributing/blob/Datascience_With_Python/Datascience_With_Python/Machine%20Learning/Algorithms/Apriori%20Algorithm/Images/table%205.png)<br>
5. The join process will take place, this time with 3-itemsets and we calculate their count and get the table shown below.
![image7](https://github.com/rachita11/winter-of-contributing/blob/Datascience_With_Python/Datascience_With_Python/Machine%20Learning/Algorithms/Apriori%20Algorithm/Images/table%206.jpg)<br>
6. We observe that only one itemset <b>{l1 l2 l3}</b> pass the criteria of having count 3 and hence hece only that itemset can be called a frequent itemset.<br>

We now apply the association rules to find if they surpass the minimum confidence value or not.
![image8](https://github.com/rachita11/winter-of-contributing/blob/Datascience_With_Python/Datascience_With_Python/Machine%20Learning/Algorithms/Apriori%20Algorithm/Images/rules.png)
This shows that all the above association rules are strong if minimum confidence threshold is 60%.

<b>Limitations of this algorithm</b><br>
1.It requires high computation if the itemsets are very large and the minimum support is kept very low.<br>
2.The entire database needs to be scanned.<br><br>



<b>Summary</b><br>
Apriori algorithm is an efficient algorithm that scans the database only once.
It reduces the size of the itemsets in the database considerably providing a good performance. Thus, data mining helps consumers and industries better in the decision-making process.<br>In this file, we saw what Apriori Algorithm really does and in what situations we can use it in, along with some of the limitations but like any other machine learning model, we can not expect a very accurate result and the final output/information derivation would always be tentative.However, it still plays an important role in giving a good rough analysis. We also have to keep in mind that if the dataset is small, the algorithm can find many false associations that happened simply by chance. You can address this issue by evaluating obtained rules on the held-out test data for the support, confidence, lift, and conviction values.
For seeing how this algorithm works in a real world problem, head over to the python file in this folder and we'll see how we can implement this algorithm.






