{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science with Python : Markov's Chain #377"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Markov's Chain ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov chains, named after <a href = \"https://en.wikipedia.org/wiki/Andrey_Markov\">Andrey Markov</a>, a stochastic model that depicts a sequence of possible events where predictions or probabilities for the next state based solely on its previous event state not the states before. In simple words, the probability that n+1 th steps will be x depends only on the nth steps not the complete sequence of steps that came before n. This property is known as <i><b>Markov Property</b></i> or <i><b>Memorylessness</b></i>.\n",
    "Let us explore our Markov chain with the help of a diagram,\n",
    "<img src = \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Markovkate_01.svg/800px-Markovkate_01.svg.png\" width=\"200\"/>\n",
    "A diagram representing a two-state(here, E and A) Markov process.Here the arrows originated from the current state and points to the future state and the number associated with the arrows indicates the probability of the Markov process changing from one state to another state. For instance,if the Markov process is in state E, then the probability it changes to state A is 0.7, while the probability it remains in same state is 0.3. Similarly, for any process in state A, probability to change to E state is 0.4 and probability to remain in same state is 0.6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Represent Markov Chain ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the diagram of the two state Markov process ,we can understand that the Markov chain is directed graph. So we can represent is with the help of an adjacency matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                 +-------+-------+         \n",
    "                 |   A   |   E   |        --- Each element denotes the probability weight of the edge                       \n",
    "         +-------+-------+-------+               connecting the two corresponding vertices                                     \n",
    "         |   A   |  0.6  |  0.4  |        --- 0.4 is the probability for state A to go to state E and 0.6 is the probability \n",
    "         +-------+-------+-------+               to remain at the same state \n",
    "         |   E   |  0.7  |  0.3  |        --- 0.7 is the probability for state E to go to state A and 0.3 is the probability \n",
    "         +-------+-------+-------+               to remain at the same state \n",
    "         \n",
    "         \n",
    "This matrix is also called <i><b>Transition Matrix</b></i>. If the Markov chain has N possible states, the matrix will be an NxN matrix. Each row of this matrix should sum to 1.\n",
    "In addition to this, a Markov chain also has an <i><b>Initial State Vector</b></i> of order Nx1.\n",
    "These two entities are must to represent a Markov chain.\n",
    "\n",
    "<i><b>N-step Transition Matrix :</b></i> Now let us learn higher order transition matrices. It helps us to find the chance of that transition occurring over multiple steps. To put in simple words, what will be the probability of moving from state <b>A</b> to state <b>E</b> over <b>N</b> step? There is actually a very simple way to calculate it. This can be determined by calculating the value of entry <b>(A,E)</b> of the matrix obtained by raising the transition matrix to the power of <b>N</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain in Python :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to code our Markov chain example above in python. Although for computing efficiently we generally use a library encoded Markov chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's import our library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'A', 1: 'E'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encoding this states to numbers as it is easier to deal with numbers instead of words.\n",
    "state = {\n",
    "    0 : \"A\",\n",
    "    1 : \"E\",\n",
    "}\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6, 0.4],\n",
       "       [0.7, 0.3]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assigning the transition matrix to a variable i.e a numpy 2d matrix.\n",
    "MyMatrix = np.array([[0.6, 0.4], [0.7, 0.3]])\n",
    "MyMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ---> A ---> A ---> A ---> A ---> E ---> A ---> E ---> A ---> A ---> A ---> E ---> E ---> E ---> A ---> E ---> A ---> A ---> A ---> E ---> stop\n"
     ]
    }
   ],
   "source": [
    "#Simulating a random walk on our Markov chain with 20 steps.\n",
    "#Random walk simply means that we start with an arbitary state and then we move along our markov chain.\n",
    "n = 20\n",
    "StartingState = 0                                     #decide which state to start with\n",
    "CurrentState = StartingState\n",
    "print(state[CurrentState], \"--->\", end=\" \")           #printing the stating state using state dictionary\n",
    "\n",
    "while n-1:\n",
    "    #Deciding the next state using a random.choice() function,that takes list of states \n",
    "    #and the probability to go to the next states from our current state\n",
    "    CurrentState = np.random.choice([0, 1], p=MyMatrix[CurrentState])\n",
    "    print(state[CurrentState], \"--->\", end=\" \")       #printing the path of random walk\n",
    "    n-=1\n",
    "print(\"stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyMatrix^n = \n",
      " [[0.63636364 0.36363636]\n",
      " [0.63636364 0.36363636]] \n",
      "\n",
      "π =  [0.63636364 0.36363636]\n"
     ]
    }
   ],
   "source": [
    "#Let us find the stationary distribution of our Markov chain using repeated matrix multiplication\n",
    "NumberOfSteps = 10**3\n",
    "MyMatrix_n = MyMatrix\n",
    "\n",
    "i=0\n",
    "while i<NumberOfSteps:\n",
    "    MyMatrix_n =  np.matmul(MyMatrix_n, MyMatrix)     #Multiplying our matrix with itself and storing it into MyMatrix_n\n",
    "    i+=1\n",
    "\n",
    "print(\"MyMatrix^n = \\n\", MyMatrix_n, \"\\n\")\n",
    "print(\"π = \", MyMatrix_n[0])                          #Printing the probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left eigen vectors = \n",
      " [[ 0.86824314 -0.70710678]\n",
      " [ 0.49613894  0.70710678]] \n",
      "\n",
      "eigen values = \n",
      " [ 1. +0.j -0.1+0.j]\n"
     ]
    }
   ],
   "source": [
    "#Let us find the stationary distribution of our Markov chain by Finding Left Eigen Vectors\n",
    "#Importing our library\n",
    "import scipy.linalg\n",
    "MyValues, left = scipy.linalg.eig(MyMatrix, right = False, left = True)    #We only need the left eigen vectors\n",
    "\n",
    "print(\"left eigen vectors = \\n\", left, \"\\n\")\n",
    "print(\"eigen values = \\n\", MyValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6363636363636364, 0.36363636363636365]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pi is a probability distribution so the sum of the probabilities should be 1\n",
    "#To get that from the above negative values we just have to normalize\n",
    "pi = left[:,0]\n",
    "pi_normalized = [(x/np.sum(pi)).real for x in pi]\n",
    "pi_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Probability Corresponding to a Particular Sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.053454545454545456\n"
     ]
    }
   ],
   "source": [
    "#How about finding P(A-->E-->E-->A)\n",
    "\n",
    "def calculate_probability(sequence, MyMatrix, pi):\n",
    "    StartingState = sequence[0]\n",
    "    prob = pi[StartingState]                                       #initializing prob with the prob of the start state\n",
    "    PreviousState, CurrentState = StartingState, StartingState\n",
    "    for i in range(1, len(sequence)):\n",
    "        CurrentState = sequence[i]\n",
    "        #Multiplying the transition prob from previous to current state with the current value of prob\n",
    "        prob *= MyMatrix[PreviousState][CurrentState]\n",
    "        PreviousState = CurrentState\n",
    "    return prob\n",
    "\n",
    "print(calculate_probability([0, 1, 1, 0], MyMatrix, pi_normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of Markov Chain :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Markov chains makes the study of many real-world processes much more simple and easy to understand. Using Markov chain we can derive some useful results such as Stationary Distribution and many more.\n",
    "2. MCMC(Markov Chain Monte Carlo) ,gives solution to the problems come from the normalization factor, is based on Markov Chain.\n",
    "3. Markov Chains are used in information theory, search engine, speech recognition etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov chain has huge possibilities, future and importance in the field of Data Science and the interested readers are requested to learn these stuff properly for being a competent person in the field of Data Science."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
