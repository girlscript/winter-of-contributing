{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7521e7c7",
   "metadata": {},
   "source": [
    "# Implement Decision Tree without using any standard ML library like scikit-learn or more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68cc04a",
   "metadata": {},
   "source": [
    "### What is Decision Tree Algorithm ? \n",
    "\n",
    "Decision Tree algorithm is one of the type of supervised learning algorithms. What makes decision tree algorithm different from other supervised learning algorithm is that it can be used for solving both regression as well as classification problems. The main aim of using a Decision Tree algorithm is to create a training model that can use to predict the class or value of the target variable by learning simple decision rules inferred from training data.\n",
    "\n",
    "It is a tree-structured classfier, where internal node represents the features of a dataset, meanwhile the branches represents the rules and every single next node represents the outcome.\n",
    "\n",
    "\n",
    "### Types of Decision Tree\n",
    "\n",
    "Types of decision trees are based on the type of target variable. It can basically be of two types:\n",
    "   1. Categorical Variable Decision Tree: Decision Tree which has a categorical target variable.\n",
    "   2. Continuous Variable Decision Tree: Decision Tree has a continuous target variable.\n",
    "   \n",
    "###### For Example:-\n",
    "A person has a problem to predict whether a customer will renew his subscription for a OTT service (yes or no). Here that person knows that the income of customers is a significant variable but the company does not have income details for all of his customers. Now, as the person know this is an important variable, then that proson can build a decision tree to predict customer income based on occupation and various other important variables. In this case, we are predicting values for the continuous variables so we need a Continuous Variable Decision Tree.\n",
    "\n",
    "\n",
    "### Important Terminology related to Decision Trees \n",
    "\n",
    "1. Root Node: It represents the entire population or sample and this further gets divided into two or more homogeneous sets.\n",
    "2. Splitting: It is a process of dividing a node into two or more sub-nodes.\n",
    "3. Decision Node: When a sub-node splits into further sub-nodes, then it is called the decision node.\n",
    "4. Leaf / Terminal Node: Nodes do not split is called Leaf or Terminal node.\n",
    "5. Pruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say the opposite process of   splitting.\n",
    "6. Branch / Sub-Tree: A subsection of the entire tree is called branch or sub-tree.\n",
    "7. Parent and Child Node: A node, which is divided into sub-nodes is called a parent node of sub-nodes whereas sub-nodes are the child of a parent node.\n",
    "\n",
    "\n",
    "### Assumptions for creating a Decision Tree.\n",
    "\n",
    "First we have to concider entire training set as the thre root node. The features are always expected to be categorical. If the data is continuous then they are discretized prior to building the model. In decision tree records are distributed recursively on the basis of attribute values and the order foracing attributes as root or internal node of the tree is done by using some statistical approach.\n",
    "\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "#### Advantages\n",
    "1. A decision tree does not require normalization of data.\n",
    "2. A decision tree does not require scaling of data.\n",
    "3. Missing values in the data doesn't affect the process to a considerable extent.\n",
    "4. A Decision tree model is very easy to explain.\n",
    "\n",
    "#### Disadvantages\n",
    "1. Tiny changes in the data can make a large difference in the structure of the decision tree causing instability.\n",
    "2. Decision Tree requires a very high time of training comparitively.\n",
    "3. For a Decision tree sometimes calculation can go far more complex compared to other algorithms.\n",
    "4. Decision tree training is relatively expensive as the complexity and time has taken are more.\n",
    "\n",
    "\n",
    "### How does Decision Trees work?\n",
    "The decision of making strategic splits heavily affects a treeâ€™s accuracy. These criterias are different for classification and regression trees. Decision trees use multiple algorithms to decide to split the one which is discussed over here is called ID3. The ID3 algorithm builds decision trees using a top-down greedy approach.\n",
    "\n",
    "#### Steps involved in ID3 algorithm:\n",
    "1. Start with the set S as the root node.\n",
    "2. On each iteration it calculates Entropy(H) and Information gain(IG) of this attribute.\n",
    "3. It then selects the attribute which has the smallest Entropy or Largest Information gain.\n",
    "4. The set S is then split by the selected attribute to produce a subset of the data.\n",
    "5. The algorithm continues to recur itself further.\n",
    "\n",
    "\n",
    "### Code for Decision Tree from scratch:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b55a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import random\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b055b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "533e58a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "040d907e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 0.88129\n"
     ]
    }
   ],
   "source": [
    "#Entropy\n",
    "from collections import Counter\n",
    "def entropy(s):\n",
    "    counts = np.bincount(s)\n",
    "    percentages = counts / len(s)\n",
    "    entropy = 0\n",
    "    for pct in percentages:\n",
    "        if pct > 0:\n",
    "            entropy += pct * np.log2(pct)\n",
    "    return -entropy\n",
    "s = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
    "print(f'Entropy: {np.round(entropy(s), 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "229f4db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain: 0.18094\n"
     ]
    }
   ],
   "source": [
    "#Information Gain\n",
    "def information_gain(parent, left_child, right_child):\n",
    "    num_left = len(left_child) / len(parent)\n",
    "    num_right = len(right_child) / len(parent)\n",
    "    \n",
    "    gain = entropy(parent) - (num_left * entropy(left_child) + num_right * entropy(right_child))\n",
    "    return gain\n",
    "\n",
    "parent = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "left_child = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
    "right_child = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "print(f'Information Gain: {np.round(information_gain(parent, left_child, right_child), 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "050a52d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Node Class\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, data_left=None, data_right=None, gain=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.data_left = data_left\n",
    "        self.data_right = data_right\n",
    "        self.gain = gain\n",
    "        self.value = value\n",
    "#Building up Decision Tree Algorithm\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=5):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "    @staticmethod\n",
    "    def _entropy(s):\n",
    "        counts = np.bincount(np.array(s, dtype=np.int64))\n",
    "        percentages = counts / len(s)\n",
    "        entropy = 0\n",
    "        for pct in percentages:\n",
    "            if pct > 0:\n",
    "                entropy += pct * np.log2(pct)\n",
    "        return -entropy\n",
    "    \n",
    "    def _information_gain(self, parent, left_child, right_child):\n",
    "        num_left = len(left_child) / len(parent)\n",
    "        num_right = len(right_child) / len(parent)\n",
    "        return self._entropy(parent) - (num_left * self._entropy(left_child) + num_right * self._entropy(right_child))\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        best_split = {}\n",
    "        best_info_gain = -1\n",
    "        n_rows, n_cols = X.shape\n",
    "        for f_idx in range(n_cols):\n",
    "            X_curr = X[:, f_idx]\n",
    "            for threshold in np.unique(X_curr):\n",
    "                df = np.concatenate((X, y.reshape(1, -1).T), axis=1)\n",
    "                df_left = np.array([row for row in df if row[f_idx] <= threshold])\n",
    "                df_right = np.array([row for row in df if row[f_idx] > threshold])\n",
    "                if len(df_left) > 0 and len(df_right) > 0:\n",
    "                    y = df[:, -1]\n",
    "                    y_left = df_left[:, -1]\n",
    "                    y_right = df_right[:, -1]\n",
    "                    gain = self._information_gain(y, y_left, y_right)\n",
    "                    if gain > best_info_gain:\n",
    "                        best_split = {\n",
    "                            'feature_index': f_idx,\n",
    "                            'threshold': threshold,\n",
    "                            'df_left': df_left,\n",
    "                            'df_right': df_right,\n",
    "                            'gain': gain\n",
    "                        }\n",
    "                        best_info_gain = gain\n",
    "        return best_split\n",
    "    \n",
    "    def _build(self, X, y, depth=0):\n",
    "        n_rows, n_cols = X.shape\n",
    "        if n_rows >= self.min_samples_split and depth <= self.max_depth:\n",
    "            best = self._best_split(X, y)\n",
    "            if best['gain'] > 0:\n",
    "                left = self._build(\n",
    "                    X=best['df_left'][:, :-1], \n",
    "                    y=best['df_left'][:, -1], \n",
    "                    depth=depth + 1\n",
    "                )\n",
    "                right = self._build(\n",
    "                    X=best['df_right'][:, :-1], \n",
    "                    y=best['df_right'][:, -1], \n",
    "                    depth=depth + 1\n",
    "                )\n",
    "                return Node(\n",
    "                    feature=best['feature_index'], \n",
    "                    threshold=best['threshold'], \n",
    "                    data_left=left, \n",
    "                    data_right=right, \n",
    "                    gain=best['gain']\n",
    "                )\n",
    "        return Node(\n",
    "            value=Counter(y).most_common(1)[0][0]\n",
    "        )\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build(X, y)\n",
    "        \n",
    "    def _predict(self, x, tree):\n",
    "        if tree.value != None:\n",
    "            return tree.value\n",
    "        feature_value = x[tree.feature]\n",
    "        if feature_value <= tree.threshold:\n",
    "            return self._predict(x=x, tree=tree.data_left)\n",
    "        if feature_value > tree.threshold:\n",
    "            return self._predict(x=x, tree=tree.data_right)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return [self._predict(x, self.root) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86f3729b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for the built model in the Iris dataset is:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTree()\n",
    "model.fit(X_train, y_train)\n",
    "predict = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predict) * 100\n",
    "print(\"Accuracy score for the built model in the Iris dataset is: \",accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3962a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
