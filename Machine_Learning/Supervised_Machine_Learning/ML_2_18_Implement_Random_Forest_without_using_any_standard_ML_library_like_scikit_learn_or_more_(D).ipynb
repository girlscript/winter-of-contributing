{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_2_18_Implement_Random_Forest_without_using_any_standard_ML_library_like_scikit-learn_or_more_(D).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwDelUkvNKVw"
      },
      "source": [
        "# Random Forest Classifier\n",
        "In this tutorial, we are going to implement Random Forest algorithm using only built-in Python modules like numpy. </br>\n",
        "The random forest algorithm is one of the most powerful and most popular\n",
        "algorithms. It is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. </br>\n",
        "<img src=\"https://i.ytimg.com/vi/goPiwckWE9M/maxresdefault.jpg\" alt=\"random_forest\">\n",
        "Image credits - https://www.youtube.com/watch?v=goPiwckWE9M</br>\n",
        "The above image shows the whole idea of Random Forest. The idea is to combine multiple trees into a forest so we train multiple trees and each tree gets a random subset of the training data thus the word random. We then make a prediction with each of the trees at the end and then we make a majority vote to\n",
        "get the final prediction. </br>\n",
        "The whole idea of the random forest has some advantages compared to only one\n",
        "tree for example by building more trees we have more chances to get the correct\n",
        "prediction and we also reduce the chance of overfitting with a single tree so\n",
        "typically the accuracy of a random forest is higher than with a single tree\n",
        "and that's why it's so powerful\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n_hBJ_CQxRf"
      },
      "source": [
        "In order to implement Random Forest Classifier, We will be using iris dataset which is available in scikit-learn library. For more information about dataset, please visit the documentation of scikit library [here](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ojWWQV3RTZC"
      },
      "source": [
        "Before implementing the algorithm, let us first explore the iris dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XupiDyhXNHYV"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Importing iris dataset from scikit learn library\n",
        "from sklearn.datasets import load_iris\n",
        "df = load_iris()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbwBYt5ORwsu",
        "outputId": "4e8ad7d0-f555-4093-9b59-1469f44d21dc"
      },
      "source": [
        "#Returns all the keys of the dataset dictionary\n",
        "df.keys()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkJYOO-_SSUj",
        "outputId": "dece6ce4-3b9a-4c60-91e2-9ade4d7a95d2"
      },
      "source": [
        "#Description of the dataset\n",
        "print(df.DESCR)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _iris_dataset:\n",
            "\n",
            "Iris plants dataset\n",
            "--------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 150 (50 in each of three classes)\n",
            "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
            "    :Attribute Information:\n",
            "        - sepal length in cm\n",
            "        - sepal width in cm\n",
            "        - petal length in cm\n",
            "        - petal width in cm\n",
            "        - class:\n",
            "                - Iris-Setosa\n",
            "                - Iris-Versicolour\n",
            "                - Iris-Virginica\n",
            "                \n",
            "    :Summary Statistics:\n",
            "\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "                    Min  Max   Mean    SD   Class Correlation\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
            "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
            "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
            "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "    :Class Distribution: 33.3% for each of 3 classes.\n",
            "    :Creator: R.A. Fisher\n",
            "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
            "    :Date: July, 1988\n",
            "\n",
            "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
            "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
            "Machine Learning Repository, which has two wrong data points.\n",
            "\n",
            "This is perhaps the best known database to be found in the\n",
            "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
            "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
            "data set contains 3 classes of 50 instances each, where each class refers to a\n",
            "type of iris plant.  One class is linearly separable from the other 2; the\n",
            "latter are NOT linearly separable from each other.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
            "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
            "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
            "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
            "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
            "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
            "     Structure and Classification Rule for Recognition in Partially Exposed\n",
            "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
            "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
            "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
            "     on Information Theory, May 1972, 431-433.\n",
            "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
            "     conceptual clustering system finds 3 classes in the data.\n",
            "   - Many, many more ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tobeCYH4SXbw",
        "outputId": "6c6f4956-ead2-412b-f66a-2f85094b4a93"
      },
      "source": [
        "# Features of Dataset (Columns of Dataset)\n",
        "print(df.feature_names)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sECuN52mSdOB",
        "outputId": "45128b5f-f498-4956-d3cc-58188fa1c15a"
      },
      "source": [
        "# Converting dictionary to dataframe and Exploring first 5 rows of Dataset with the help of pandas library\n",
        "iris = pd.DataFrame(df.data,columns = df.feature_names)\n",
        "iris.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
              "0                5.1               3.5                1.4               0.2\n",
              "1                4.9               3.0                1.4               0.2\n",
              "2                4.7               3.2                1.3               0.2\n",
              "3                4.6               3.1                1.5               0.2\n",
              "4                5.0               3.6                1.4               0.2"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MnBAVJ41Sj28",
        "outputId": "6f53c21d-d0dd-4ad2-e12d-d774b16771f5"
      },
      "source": [
        "# Adding a new column of target values to iris dataframe (class of iris flower)\n",
        "iris['TARGET_VALUE'] = df.target\n",
        "iris.tail()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>TARGET_VALUE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sepal length (cm)  sepal width (cm)  ...  petal width (cm)  TARGET_VALUE\n",
              "145                6.7               3.0  ...               2.3             2\n",
              "146                6.3               2.5  ...               1.9             2\n",
              "147                6.5               3.0  ...               2.0             2\n",
              "148                6.2               3.4  ...               2.3             2\n",
              "149                5.9               3.0  ...               1.8             2\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1Snd7s-S2Kw",
        "outputId": "8968ed67-3bee-4f4f-c47d-db48c123b7f4"
      },
      "source": [
        "# Checking if the dataset contains any null values or not.\n",
        "iris.isnull().sum()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sepal length (cm)    0\n",
              "sepal width (cm)     0\n",
              "petal length (cm)    0\n",
              "petal width (cm)     0\n",
              "TARGET_VALUE         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLj9mvrjTH1a",
        "outputId": "5d5a890b-6768-4ba6-b20a-83528615ff28"
      },
      "source": [
        "iris.value_counts(\"TARGET_VALUE\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TARGET_VALUE\n",
              "2    50\n",
              "1    50\n",
              "0    50\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfWAcQajU2C4"
      },
      "source": [
        "Here,</br>\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>TARGET_VALUE</th>\n",
        "    <th>TARGET_NAME</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> 2 </td>\n",
        "    <td> Virginica </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> 1 </td>\n",
        "    <td> Versicolor </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> 0 </td>\n",
        "    <td> Setosa </td>\n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLHcBLszXT3E"
      },
      "source": [
        "DATA VISUALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "mpTzspQxUZ4u",
        "outputId": "ccbfb3b2-0da9-40b8-ad59-cad2ba9066e0"
      },
      "source": [
        "# Extract the values for features and create a list called featuresAll\n",
        "featuresAll=[]\n",
        "features = df.data[: , [0,1,2,3]]\n",
        "\n",
        "# Extract the values for targets\n",
        "targets = df.target\n",
        "targets.reshape(targets.shape[0],-1)\n",
        "\n",
        "# Every observation gets appended into the list once it is read. For loop is used for iteration process\n",
        "for observation in features:\n",
        "    featuresAll.append([observation[0] + observation[1] + observation[2] + observation[3]])\n",
        "\n",
        "# Plotting the Scatter plot\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(featuresAll, targets, color='red', alpha =1.0)\n",
        "plt.rcParams['figure.figsize'] = [10,8]\n",
        "plt.title('Iris Dataset scatter Plot')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Targets')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Targets')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBn1_37KwDUF"
      },
      "source": [
        "Since Random Forest was built on top of Deccision Tree algorithm. \n",
        "The math behind Decision tree is summarised in the followin picture:\n",
        "<img src=\"https://i.pinimg.com/originals/ac/10/0a/ac100a03e2d1ad9a1132b5b4939a41a1.png\" alt=\"formula\">\n",
        "Let us first build Decision tree based on the formula shown above </br>\n",
        "Image credits - https://www.pinterest.it/pin/326511041737648188/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yENjINU1zPDi"
      },
      "source": [
        "Algorithm approached for building Decision Tree:\n",
        "Train Algorithm := Build the tree\n",
        "1.  Start at the top node and at each node select the best split based on the best information gain\n",
        "2.   Greedy Search: Loop over all features and over all thresholds (all possible feature values)\n",
        "3. Save the best split feature and split the threshold at each node.\n",
        "4. Build the tree recursively\n",
        "5. Applt some stopping criteria to stop growing\n",
        "e.g. here: maximum depth, minimum samples at node, no more class distribution in node\n",
        "5. when we have a leaf node, store the most common class label of this node \n",
        "\n",
        "Predict := Traverse tree\n",
        "1.   Traverse the tree recursively.\n",
        "2.   At each node look at the best split feature vector x and go left or right\n",
        "depending on x[feature_idx] <= threshold\n",
        "3. When we reach the leaf node we return the stored most common class label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_08F0FiX7xX"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def entropy(y):\n",
        "  #calculate number of occurences of all class labels\n",
        "  hist = np.bincount(y)\n",
        "  #probability\n",
        "  ps = hist / len(y)\n",
        "  #Formula for Entropy\n",
        "  return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
        "\n",
        "\n",
        "#Store the information of our Node\n",
        "class Node:\n",
        "  def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
        "    self.feature = feature\n",
        "    self.threshold = threshold\n",
        "    self.left = left\n",
        "    self.right = right\n",
        "    self.value = value\n",
        "\n",
        "  #If we have a value, we are at leaf node, otherwise not\n",
        "  def is_leaf_node(self):\n",
        "    return self.value is not None\n",
        "\n",
        "class DecisionTree:\n",
        "  #We do greedy search over all the features\n",
        "  def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n",
        "    self.min_samples_split = min_samples_split\n",
        "    self.max_depth = max_depth\n",
        "    self.n_feats = n_feats\n",
        "    self.root = None\n",
        "\n",
        "  #Fit Training data\n",
        "  def fit(self, X, y):\n",
        "    #Grow tree\n",
        "    self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n",
        "    self.root = self._grow_tree(X, y)\n",
        "\n",
        "  #Make predictions\n",
        "  def predict(self, X):\n",
        "    return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "  def _grow_tree(self, X, y, depth=0):\n",
        "    n_samples, n_features = X.shape\n",
        "    n_labels = len(np.unique(y))\n",
        "\n",
        "    # stopping criteria\n",
        "    if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n",
        "      leaf_value = self._most_common_label(y)\n",
        "      return Node(value=leaf_value)\n",
        "\n",
        "    feat_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n",
        "\n",
        "    # greedily select the best split according to information gain\n",
        "    best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n",
        "\n",
        "    # grow the children that result from the split\n",
        "    left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
        "    left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
        "    right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
        "    # return new node in the middle\n",
        "    return Node(best_feat, best_thresh, left, right)\n",
        "\n",
        "  #To select best criteria to split the data further\n",
        "  def _best_criteria(self, X, y, feat_idxs):\n",
        "    best_gain = -1\n",
        "    split_idx, split_thresh = None, None\n",
        "    for feat_idx in feat_idxs:\n",
        "      X_column = X[:, feat_idx]\n",
        "      thresholds = np.unique(X_column)\n",
        "      for threshold in thresholds:\n",
        "        gain = self._information_gain(y, X_column, threshold)\n",
        "\n",
        "        if gain > best_gain:\n",
        "          best_gain = gain\n",
        "          split_idx = feat_idx\n",
        "          split_thresh = threshold\n",
        "\n",
        "    return split_idx, split_thresh\n",
        "\n",
        "  def _information_gain(self, y, X_column, split_thresh):\n",
        "    # parent loss\n",
        "    parent_entropy = entropy(y)\n",
        "\n",
        "    # generate split\n",
        "    left_idxs, right_idxs = self._split(X_column, split_thresh)\n",
        "\n",
        "    if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "      return 0\n",
        "\n",
        "    # compute the weighted avg. of the loss for the children\n",
        "    n = len(y)\n",
        "    n_l, n_r = len(left_idxs), len(right_idxs)\n",
        "    e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n",
        "    child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
        "\n",
        "    # information gain is difference in loss before vs. after split\n",
        "    ##########FORMULA FOR INFORMATION GAIN\n",
        "    #IG = E(parent) - [weighted average] - E(children)\n",
        "    ig = parent_entropy - child_entropy\n",
        "    return ig\n",
        "\n",
        "  def _split(self, X_column, split_thresh):\n",
        "    left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
        "    right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "    return left_idxs, right_idxs\n",
        "\n",
        "  def _traverse_tree(self, x, node):\n",
        "    if node.is_leaf_node():\n",
        "      return node.value\n",
        "\n",
        "    if x[node.feature] <= node.threshold:\n",
        "      #left Tree\n",
        "      return self._traverse_tree(x, node.left)\n",
        "    #Right Tree\n",
        "    return self._traverse_tree(x, node.right)\n",
        "\n",
        "  def _most_common_label(self, y):\n",
        "    counter = Counter(y)\n",
        "    most_common = counter.most_common(1)[0][0]\n",
        "    return most_common\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87ghDm-KzscC"
      },
      "source": [
        "Building Random Forest Algorithm with the help of Decision tree algorithm implemented above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJM0GFpwxUyf"
      },
      "source": [
        "#To give our tree random subset\n",
        "def bootstrap_sample(X, y):\n",
        "  n_samples = X.shape[0]\n",
        "  idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
        "  return X[idxs], y[idxs]\n",
        "\n",
        "def most_common_label(y):\n",
        "  counter = Counter(y)\n",
        "  most_common = counter.most_common(1)[0][0]\n",
        "  return most_common\n",
        "\n",
        "class RandomForest:\n",
        "  def __init__(self, n_trees=10, min_samples_split=2, max_depth=100, n_feats=None):\n",
        "    self.n_trees = n_trees\n",
        "    self.min_samples_split = min_samples_split\n",
        "    self.max_depth = max_depth\n",
        "    self.n_feats = n_feats\n",
        "    #To store each single tree that we create \n",
        "    self.trees = []\n",
        "  \n",
        "  #For fitting training data\n",
        "  def fit(self, X, y):\n",
        "    self.trees = []\n",
        "    for _ in range(self.n_trees):\n",
        "      tree = DecisionTree(min_samples_split=self.min_samples_split,max_depth=self.max_depth,n_feats=self.n_feats,)\n",
        "      X_samp, y_samp = bootstrap_sample(X, y)\n",
        "      tree.fit(X_samp, y_samp)\n",
        "      self.trees.append(tree)\n",
        "\n",
        "  #For predictions\n",
        "  def predict(self, X):\n",
        "    tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "    tree_preds = np.swapaxes(tree_preds, 0, 1)\n",
        "    # To perform majority voting\n",
        "    y_pred = [most_common_label(tree_pred) for tree_pred in tree_preds]\n",
        "    return np.array(y_pred)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlKk1Nza0B_b"
      },
      "source": [
        "Finally Fitting iris data set and making predictions using the Random Forest Algorithm that we have implemented without using any standard ML library like scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrUbP28u0Ai0",
        "outputId": "056f7cfb-dfe4-4449-aa93-6f2778b763dc"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Imports\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    #To calculate accuracy predicted by our model\n",
        "    def accuracy(y_true, y_pred):\n",
        "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "        return accuracy\n",
        "\n",
        "    X = df.data\n",
        "    y = df.target\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "    clf = RandomForest(n_trees=3, max_depth=10)\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    acc = accuracy(y_test, y_pred)\n",
        "\n",
        "    print(\"Accuracy obtained from Random Forest Classifier is:\", acc)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy obtained from Random Forest Classifier is: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3z66Qzg4Tz3"
      },
      "source": [
        "References:\n",
        "1. https://youtu.be/Oq1cKjR8hNo\n",
        "2. https://www.kaggle.com/lalitharajesh/iris-dataset-exploratory-data-analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoBmxvFm7DdK"
      },
      "source": [
        "Contributor</br>\n",
        "ID : 992</br>\n",
        "ML BATCH : 6</br>\n",
        "[@NikitaEmberi](https://github.com/NikitaEmberi)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOuouJtH7YPN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}